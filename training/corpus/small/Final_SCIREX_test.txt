Pixels O
in O
these O
scene O
images O
are O
annotated O
regarding O
many O
stuff O
and O
objects O
. O

section O
: O
Acknowledgments O

However O
, O
none O
of O
the O
aforementioned O
works O
have O
parsed O
objects O
at O
an O
instance O
level O
as O
shown O
in O
Fig O
. O
1 O
, O
but O
rather O
category O
level O
. O

Thus O
, O
we O
can O
see O
that O
with O
the O
optimization B-Method
process I-Method
in O
the O
back B-Method
propagation I-Method
training I-Method
process I-Method
, O
aNMM B-Method
can O
learn O
better O
question B-Metric
term I-Metric
weighting I-Metric
score I-Metric
than O
heuristic B-Method
term I-Method
weighting I-Method
functions I-Method
like O
IDF B-Method
. O

We O
evaluate O
our O
model O
and O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
in O
sequence B-Task
modeling I-Task
tasks I-Task
on O
two O
benchmark O
datasets O
– O
Penn B-Material
Treebank I-Material
and O

Cascaded B-Method
classifiers I-Method
are O
a O
well O
- O
studied O
technique O
in O
Computer B-Task
Vision I-Task
. O

Since O
the O
hyperplanes O
for O
the O
split O
- O
nodes O
of O
a O
decision O
tree O
are O
required O
to O
be O
orthogonal O
to O
each O
other O
, O
seeking O
an O
optimal O
orthogonal O
hyperplane O
locally O
can O
not O
guarantee O
obtaining O
maximum O
purity O
for O
the O
whole O
tree O
globally O
. O

In O
addition O
, O
we O
employ O
Online B-Task
Hard I-Task
Example I-Task
Mining I-Task
( O
OHEM B-Method
) I-Method
, O
and O
only O
compute O
our O
loss B-Metric
over O
the O
top O
K O
pixels O
with O
the O
highest O
loss O
in O
the O
training O
mini O
- O
batch O
. O

Here O
we O
introduce O
a O
few O
extensions O
of O
QRN B-Method
, O
and O
later O
in O
our O
experiments O
, O
we O
test O
QRN B-Method
's O
performance O
with O
and O
without O
each O
of O
these O
extensions O
. O

Practically O
, O
DeepWalk B-Method
only O
applies O
to O
unweighted B-Task
networks I-Task
, O
while O
our O
model O
is O
applicable O
for O
networks O
with O
both O
weighted B-Task
and O
unweighted B-Task
edges I-Task
. O

( O
c O
) O
) O
. O

section O
: O
Sequential B-Task
Prediction I-Task
with O
Learned O
Spatial O
Context O
Features O

Wu O
et O
al O
. O
combine O
CNN B-Method
embeddings O
with O
the O
hand O
- O
crafted O
features O
in O
the O
FC B-Method
layer O
. O

All O
our O
models O
are O
trained O
on O
the O
union O
set O
of O
80k O
training O
set O
and O
40k O
validation O
set O
, O
and O
evaluated O
on O
20k O
test O
- O
dev O
set O
. O

section O
: O
Conclusions O

Training O
deep B-Method
ConvNet I-Method
detectors I-Method
with O
SGD B-Method
typically O
requires O
hundreds O
of O
thousands O
of O
SGD B-Method
steps I-Method
and O
freezing O
the O
model O
for O
even O
a O
few O
iterations O
at O
a O
time O
would O
dramatically O
slow O
progress O
. O

section O
: O
Very O
deep O
RED B-Method
- I-Method
Net I-Method
for O
Image B-Task
Restoration I-Task

While O
they O
offer O
speed B-Metric
and O
accuracy B-Metric
improvements O
over O
R B-Method
- I-Method
CNN I-Method
, O
both O
still O
fall O
short O
of O
real O
- O
time O
performance O
. O

This O
is O
especially O
true O
for O
weight B-Task
dropout I-Task
since O
in O
this O
case O
dropped O
weights O
do O
not O
get O
updated O
in O
the O
training O
iteration O
. O

This O
tendency O
to O
output O
pillow O
also O
explains O
why O
the O
source B-Method
only I-Method
model I-Method
achieves O
such O
abnormally O
high O
accuracy B-Metric
on O
the O
pillow O
class O
, O
despite O
poor O
performance O
on O
the O
rest O
of O
the O
classes O
. O

Thus O
, O
we O
utilize O
this O
property O
to O
adapt O
low O
- O
dimensional O
softmax O
outputs O
of O
segmentation O
predictions O
via O
an O
adversarial B-Method
learning I-Method
scheme I-Method
. O

Conditional B-Task
computation I-Task
, O
where O
parts O
of O
the O
network O
are O
active O
on O
a O
per O
- O
example O
basis O
, O
has O
been O
proposed O
in O
theory O
as O
a O
way O
of O
dramatically O
increasing O
model B-Task
capacity I-Task
without O
a O
proportional O
increase O
in O
computation B-Task
. O

We O
have O
shown O
the O
merits O
of O
adopting O
adversarial B-Method
learning I-Method
in O
the O
output O
space O
. O

We O
define O
an O
additional O
loss O
L O
importance O
, O
which O
is O
added O
to O
the O
overall O
loss B-Metric
function I-Metric
for O
the O
model O
. O

We O
compare O
16 O
variants O
from O
four O
aspects O
, O
i.e. O
, O
different O
baselines O
( O
Mask B-Method
R I-Method
- I-Method
CNN I-Method
and O
MH B-Method
- I-Method
Parser I-Method
) O
, O
different O
network B-Method
structures I-Method
( O
w O
/ O
o O
, O
w O
/ O
o O
concatenated O
input O
( O
RGB O
only O
) O
, O
w O
/ O
o O
concatenated O
input O
( O
RGB O
only O
) O
, O
w O
/ O
o O
, O
w O
/ O
o O
, O
w O
/ O
o O
concatenated O
input O
, O
w O
/ O
o O
, O
w O
/ O
o O
concatenated O
input O
, O
w O
/ O
o O
MSFU B-Method
) O
, O
our O
proposed O
NAN B-Method
, O
and O
upperbounds O
( O
: O
use O
the O
ground O
truth O
semantic O
saliency O
maps O
instead O
of O
prediction O
while O
keeping O
other O
settings O
the O
same O
; O
: O
use O
the O
ground B-Method
truth I-Method
instance I-Method
- I-Method
agnostic I-Method
parsing I-Method
maps I-Method
instead O
of O
prediction O
while O
keeping O
other O
settings O
the O
same O
; O
: O
use O
the O
ground O
truth O
instance O
number O
instead O
of O
prediction O
while O
keeping O
other O
settings O
the O
same O
; O
: O
use O
the O
ground O
truth O
pixel O
- O
wise O
instance O
location O
maps O
instead O
of O
prediction O
while O
keeping O
other O
settings O
the O
same O
) O
. O

As O
in O
the O
main O
text O
, O
the O
same O
observations O
about O
our O
approach O
apply O
here O
. O

The O
reason O
might O
be O
that O
there O
are O
much O
fewer O
training O
examples O
for O
large O
angles O
than O
for O
other O
angles O
. O

In O
this O
work O
, O
for O
a O
given O
test O
template O
of O
the O
IJB B-Material
- I-Material
A I-Material
data O
we O
perform O
two O
kinds O
of O
pooling B-Method
to O
produce O
its O
final O
representation O
: O
Average B-Method
pooling I-Method
( O
CNN B-Method
) O
: O

We O
provide O
an O
experimental O
evaluation O
of O
the O
proposed O
domain B-Method
- I-Method
adversarial I-Method
learning I-Method
idea I-Method
over O
a O
range O
of O
deep B-Method
architectures I-Method
and O
applications O
. O

In O
this O
paper O
, O
we O
introduce O
a O
novel O
attentive B-Method
node I-Method
composition I-Method
function I-Method
that O
is O
based O
on O
S B-Method
- I-Method
LSTM I-Method
. O

Bidirectional B-Method
RNNs I-Method
( O
BRNNs B-Method
) O
do O
this O
by O
processing O
the O
data O
in O
both O
directions O
with O
two O
separate O
hidden O
layers O
, O
which O
are O
then O
fed O
forwards O
to O
the O
same O
output O
layer O
. O

[ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
][ O
reference O
] O
. O

– O

The O
corpus O
size O
varies O
from O
few O
thousand O
sentences O
( O
e.g. O
Armenian O
or O
Kazakh O
) O
to O
more O
than O
50 O
million O
( O
e.g. O
Spanish B-Material
or O
Romanian O
) O
. O

Iterative B-Method
bbox I-Method
regression I-Method
( O
B O
) O
further O
improves O
the O
FRCN B-Method
mAP I-Metric
to O
72.4 O
% O
. O

Even O
though O
our O
network O
is O
trained O
using O
square O
images O
only O
, O
it O
generalizes O
well O
to O
other O
aspect O
ratios O
. O

The O
rest O
of O
each O
image O
is O
considered O
as O
background O
. O

A O
momentum O
term O
of O
0.9 O
is O
also O
used O
. O

This O
work O
is O
supported O
by O
the O
NSF O
award O
IIS O
# O
1566511 O
, O
a O
gift O
from O
Adobe O
Systems O
Inc. O
, O
and O
a O
GPU B-Method
from O
NVIDIA B-Method
. O

We O
compared O
the O
models O
when O
grounding O
only O
the O
hypothesis O
[ O
H O
+ O
I O
] O
, O
while O
leaving O
out O
the O
premise O
. O

Recent O
developments O
in O
machine B-Task
translation I-Task
have O
also O
shown O
that O
text O
of O
varying O
length O
can O
be O
represented O
as O
a O
fixed B-Method
- I-Method
size I-Method
vector I-Method
using O
convolutional B-Method
networks I-Method
or O
recurrent B-Method
neural I-Method
networks I-Method
. O

section O
: O
Related O
Work O

Many O
transition B-Method
systems I-Method
exist O
in O
the O
literature O
. O

In O
contrast O
, O
if O
the O
parameters O
in O
all O
output O
softmax O
layers O
were O
the O
same O
, O
the O
method O
would O
degenerate O
to O
self B-Method
- I-Method
training I-Method
. O

For O
this O
problem O
, O
we O
find O
the O
range O
of O
the O
target O
( O
e.g. O
, O
) O
, O
quantize O
it O
into O
a O
complete O
and O
ordered O
label O
set O
, O
where O
is O
the O
label O
set O
size O
and O
are O
all O
possible O
predictions O
for O
. O

However O
, O
when O
training O
a O
split O
node O
in O
a O
decision O
tree O
of O
an O
RF B-Method
, O
only O
one O
or O
a O
few O
of O
the O
feature O
dimensions O
are O
chosen O
as O
candidate O
features O
for O
comparison O
. O

; O
matching B-Task
, O
below O
of O
= O
mi O
- O
14 O
, O
node O
distance=1.5cm*1.5 O
] O

We O
propose O
a O
conditional B-Method
adversarial I-Method
framework I-Method
for O
generating O
high B-Task
- I-Task
resolution I-Task
photo I-Task
- I-Task
realistic I-Task
images I-Task
from O
semantic B-Task
label I-Task
maps I-Task
. O

We O
have O
ran O
several O
control O
experiments O
( O
for O
example O
see O
figure O
[ O
reference O
] O
) O
and O
using O
linear O
activation O
was O
always O
inferior O
to O
using O
rectified B-Method
linear I-Method
units I-Method
in O
all O
stages O
of O
the O
factorization B-Task
. O

As O
we O
can O
see O
, O
TBCNN B-Method
is O
more O
robust O
than O
sequential B-Method
convolution I-Method
in O
terms O
of O
word O
order O
distortion O
, O
which O
may O
be O
introduced O
by O
determinators O
, O
modifiers O
, O
etc O
. O

Although O
it O
is O
also O
designed O
for O
outputting O
a O
sequence O
, O
it O
is O
essentially O
a O
prediction B-Method
model I-Method
that O
learns O
to O
predict O
a O
sequence O
embedded O
in O
graph O
while O
our O
approach O
is O
a O
generative B-Method
model I-Method
that O
learns O
a O
mapping O
between O
graph O
inputs O
and O
sequence O
outputs O
. O

A O
QRN B-Method
unit O
accepts O
two O
inputs O
( O
local O
query O
vector O
q O
t O
∈ O
R O
d O
and O
sentence O
vector O
x O
t O
∈ O
R O
d O
) O
, O
and O
two O
outputs O
( O
reduced O
query O
vector O
h O
t O
∈ O
R O
d O
, O
which O
is O
similar O
to O
the O
hidden O
state O
in O
RNN B-Method
, O
and O
the O
sentence O
vector O
x O
t O
from O
the O
input O
without O
modification O
) O
. O

The O
second O
- O
order O
proximity O
between O
a O
pair O
of O
vertices O
( O
u O
, O
v O
) O
in O
a O
network O
is O
the O
similarity O
between O
their O
neighborhood O
network O
structures O
. O

Correspondence O
should O
be O
addressed O
to O
Raffaella O
Bernardi O
( O
raffaella.bernardi@unitn.it O
) O
and O
Albert O
Gatt O
( O
albert.gatt@um.edu.mt O
) O
. O

Flickr B-Method
Network I-Method
. O

In O
the O
reconstructed B-Method
network I-Method
, O
the O
LINE B-Method
( I-Method
2nd I-Method
) I-Method
outperforms O
DeepWalk B-Method
in O
most O
cases O
. O

and O
the O
new O
projection O
matrix O
can O
be O
derived O
as O

We O
place O
prediction B-Method
layers I-Method
for O
shoulders O
at O
conv4_8 O
, O
for O
elbows O
and O
hips O
at O
conv4_14 O
and O
for O
knees O
at O
conv4_18 O
. O

Additionally O
, O
we O
build O
a O
motion B-Method
compensation I-Method
scheme I-Method
based O
on O
spatial B-Method
transformers I-Method
, O
which O
is O
combined O
with O
spatio B-Method
- I-Method
temporal I-Method
models I-Method
to O
lead O
to O
a O
very O
efficient O
solution O
for O
video B-Task
SR O
with O
motion B-Task
compensation I-Task
that O
is O
end O
- O
to O
- O
end O
trainable O
. O

All O
of O
these O
metrics O
were O
averaged O
over O
several O
training O
batches O
. O

Dialog O
. O

Further O
, O
when O
trained O
on O
vast O
amounts O
of O
data O
, O
language B-Task
models I-Task
compactly O
extract O
knowledge O
encoded O
in O
the O
training O
data O
. O

One O
can O
see O
similar O
effects O
when O
using O
linear B-Method
activations I-Method
for O
the O
dimension B-Method
reduction I-Method
components I-Method
. O

2 O
) O
the O
learned O
embedding B-Method
of O
pedestrian O
using O
a O
convolutional B-Method
neural I-Method
network I-Method
( O
CNN B-Method
) O
. O

Table O
[ O
reference O
] O
shows O
how O
our O
final O
model O
compares O
against O
the O
models O
by O
Moreno O
- O
Nouguer O
and O
Martinez O
et O
al O
. O
. O

Note O
that O
with O
the O
supervised B-Task
task I-Task
, O
it O
is O
feasible O
to O
concatenate O
the O
embeddings O
learned O
with O
LINE B-Method
( I-Method
1st I-Method
) I-Method
and O
LINE B-Method
( I-Method
2nd I-Method
) I-Method
. O

As O
averages O
the O
IoU O
of O
each O
semantic O
part O
category O
, O
it O
fails O
to O
reflect O
how O
many O
semantic O
parts O
are O
correctly O
parsed O
. O

section O
: O
Unified B-Task
Detection I-Task

New O
vertices O
. O

[ O
Alignment O
] O
We O
define O
for O
both O
datasets O
. O

t. O

Approaches O
have O
therefore O
been O
developed O
to O
overcome O
the O
limitations O
. O

The O
images O
can O
be O
classified O
into O
two O
types O
, O
, O
cropped O
images O
and O
images O
of O
pedestrians O
automatically O
detected O
by O
the O
DPM B-Method
. O

Our O
approach O
is O
directly O
inspired O
by O
the O
theory O
on O
domain B-Method
adaptation I-Method
suggesting O
that O
, O
for O
effective O
domain B-Task
transfer I-Task
to O
be O
achieved O
, O
predictions B-Task
must O
be O
made O
based O
on O
features O
that O
can O
not O
discriminate O
between O
the O
training O
( O
source O
) O
and O
test O
( O
target O
) O
domains O
. O

The O
advantage O
of O
GAN B-Method
over O
other O
generative B-Method
methods I-Method
is O
that O
there O
is O
no O
need O
for O
complex O
sampling O
or O
inference B-Task
during O
training B-Task
; O
the O
downside O
is O
that O
it O
may O
be O
difficult O
to O
train O
. O

It O
is O
essential O
to O
build O
upon O
a O
good O
baseline B-Method
model I-Method
to O
achieve O
high O
- O
quality O
segmentation B-Task
results O
. O

Linear O
behavior O
in O
high O
- O
dimensional O
spaces O
is O
sufficient O
to O
cause O
adversarial O
examples O
. O

The O
task O
is O
created O
by O
taking O
the O
idea O
of O
data B-Task
augmentation I-Task
– O
which O
is O
commonly O
used O
in O
supervised B-Task
learning I-Task
– O
to O
the O
extreme O
. O

Our O
hypothesis O
based O
on O
linearity O
is O
simpler O
, O
and O
can O
also O
explain O
why O
softmax B-Method
regression I-Method
is O
vulnerable O
to O
adversarial O
examples O
. O

More O
information O
can O
be O
found O
in O
his O
homepage O
: O
. O

In O
Section O
[ O
reference O
] O
, O
we O
discuss O
some O
studies O
have O
been O
done O
which O
are O
related O
to O
our O
paper O
. O

In O
this O
work O
, O
we O
fix O
input O
image O
size O
as O
512 O
256 O
. O

– O

Despite O
significant O
progress O
made O
over O
the O
past O
twenty O
five O
years O
, O
unconstrained B-Task
face I-Task
verification I-Task
remains O
a O
challenging O
problem O
. O

Barbara O
is O
supported O
by O
NVIDIA O
corporation O
and O
thanks O
the O
Computing O
Center O
of O
the O
University O
of O
Groningen O
for O
HPC O
support O
. O

[ O
reference O
] O
intuitively O
shows O
four O
examples O
of O
label B-Task
distribution I-Task
for O
different O
recognition B-Task
tasks I-Task
. O

subsection O
: O
Basic O
MIL B-Method
network O

Indeed O
, O
we O
see O
that O
adding O
the O
same O
data O
multiple O
times O
can O
sometimes O
harm O
performance O
, O
since O
although O
it O
increases O
recency O
this O
comes O
at O
the O
expense O
of O
diversity O
. O

Then O
a O
kernel B-Method
applies O
a O
convolutional B-Method
operation I-Method
to O
a O
window O
size O
of O
words O
to O
produce O
a O
new O
feature O
map O
: O
where O
operator O
is O
the O
summation B-Method
of I-Method
elementwise I-Method
production I-Method
, O
is O
a O
bias O
term O
and O
is O
a O
non B-Method
- I-Method
linear I-Method
function I-Method
. O

Song O
et O
al O
. O
develop O
a O
technology O
to O
discover O
frequent O
discriminative O
configurations O
of O
visual O
patterns O
for O
robust B-Task
WSOD I-Task
. O

The O
celebrated O
Sequence B-Method
to I-Method
Sequence I-Method
learning I-Method
( O
Seq2Seq B-Method
) O
technique O
and O
its O
numerous O
variants O
achieve O
excellent O
performance O
on O
many O
tasks O
such O
as O
Neural B-Task
Machine I-Task
Translation I-Task
bahdanau2014neural O
, O
gehring2017convolutional O
, O
Natural B-Task
Language I-Task
Generation I-Task
( O
NLG B-Task
) O

Based O
on O
the O
Tikhonov B-Method
regularization I-Method
/ I-Method
ridge I-Method
- I-Method
regression I-Method
theory I-Method
, O
the O
closed B-Method
- I-Method
form I-Method
solution I-Method
of O
the O
coefficients O
is O
given O
: O

As O
far O
as O
we O
know O
, O
these O
architectures O
have O
never O
been O
adopted O
for O
3D B-Task
reconstruction I-Task
from O
single O
images O
, O
though O
graph B-Method
and I-Method
surface I-Method
manifold I-Method
are O
natural O
representations O
for O
meshed O
objects O
. O

section O
: O
Experiments O
: O

Then O
in O
order O
to O
increase O
the O
chance O
of O
passing O
Turing O
Test O
, O
we O
actually O
need O
to O
minimize O
the O
exact O
opposite O
average O
negative O
log O
- O
likelihood O
, O
with O
the O
role O
of O
and O
exchanged O
. O

The O
learning O
dynamics O
curves O
are O
shown O
in O
Figure O
[ O
reference O
] O
. O

We O
build O
an O
anchor B-Method
- I-Method
based I-Method
deep I-Method
face I-Method
detector I-Method
, O
which O
only O
output O
a O
single O
feature O
map O
with O
small O
anchors O
, O
to O
specifically O
learn O
small O
faces O
and O
train O
it O
by O
a O
novel O
hard B-Method
image I-Method
mining I-Method
strategy I-Method
. O

This O
result O
is O
expected O
since O
for O
larger O
graphs O
it O
is O
more O
difficult O
for O
the O
encoder B-Method
to O
compress O
all O
necessary O
information O
into O
a O
fixed O
- O
length O
vector O
; O
as O
intended O
, O
applying O
the O
attention B-Method
mechanism I-Method
in O
decoding B-Task
enabled O
our O
proposed O
Graph2Seq B-Method
model I-Method
to O
successfully O
handle O
large O
graphs O
. O

[ O
reference O
] O
, O
and O
Differentiable B-Method
Neural I-Method
Computer I-Method
( O
DNC B-Method
) O

subsection O
: O
Overview O
of O
the O
Proposed O
Model O

It O
is O
clear O
that O
our O
parsers B-Method
are O
very O
competitive O
, O
despite O
using O
very O
simple O
parsing B-Method
architectures I-Method
and O
minimal B-Method
feature I-Method
extractors I-Method
. O

: O
we O
use O
LSTM B-Method
for O
the O
leaf O
node O
function O
. O

We O
tried O
both O
“ O
hard O
” O
( O
where O
the O
student O
ignores O
low O
- O
confidence O
examples O
) O
and O
“ O
soft O
” O
( O
where O
examples O
are O
weighted O
according O
to O
the O
teacher O

More O
challenging O
datasets O
have O
since O
been O
collected O
. O

The O
most O
significant O
improvement O
is O
for O
ankles O
( O
MOTA B-Task
) O
. O

As O
the O
sparse O
constraint O
in O
[ O
reference O
] O
is O
still O
a O
bottleneck O
on O
training B-Task
dictionaries I-Task
considering O
the O
computation B-Task
, O
an O
intuitive O
way O
to O
solve O
it O
is O
to O
relax O
the O
constraint O
again O
to O
2 O
- O
norm O
. O

Depending O
on O
the O
number O
recurrent O
states O
, O
we O
describe O
RNNs B-Method
as O
‘ O
‘ O
single O
- O
state O
’ O
’ O
( O
i.e. O
one O
recurrent O
state O
) O
or O
‘ O
‘ O
dual O
- O
state O
’ O
’ O
( O
i.e. O
two O
recurrent O
states O
) O
. O

denotes O
the O
sum O
of O
all O
matching O
signals O
within O
the O
- O
th O
value O
range O
or O
bin O
. O

Using O
the O
same O
model O
trained O
as O
above O
( O
using O
SS B-Method
) O
, O
we O
test O
proposals O
generated O
by O
EdgeBoxes O
only O
. O

To O
ensure O
the O
features O
are O
consistent O
within O
each O
instance O
, O
we O
add O
an O
instance B-Method
- I-Method
wise I-Method
average I-Method
pooling I-Method
layer I-Method
to O
the O
output O
of O
the O
encoder O
to O
compute O
the O
average O
feature O
for O
the O
object O
instance O
. O

[ O
reference O
] O
. O

Fig O
. O

Evaluation B-Task
of I-Task
error I-Task
correction I-Task
is O
also O
highly O
subjective O
and O
human O
annotators O
have O
rather O
low O
agreement O
on O
gold O
- O
standard O
corrections O
. O

For O
both O
of O
the O
generator B-Method
and O
the O
discriminator O
costs O
, O
the O
momentum O
values O
were O
set O
to O
0.1 O
and O
the O
learning B-Metric
rates I-Metric
were O
set O
to O
0.1 O
. O

In O
addition O
, O
the O
dataset O
has O
very O
few O
examples O
for O
certain O
classes O
, O
such O
as O
toilet O
and O
bathtub O
, O
which O
directly O
translates O
to O
reduced O
classification B-Metric
performance I-Metric
. O

K O
dataset O
[ O
] O
: O
training O
images O
and O
test O
images O
with O
varying O
image O
sizes O
. O

Skip O
connections O
are O
passed O
every O
two O
convolutional B-Method
layers I-Method
to O
their O
mirrored B-Method
deconvolutional I-Method
layers I-Method
. O

In O
contrast O
, O
the O
Bi O
- O
LSTM B-Method
is O
a O
single O
model O
for O
detecting O
all O
error B-Task
types I-Task
, O
and O
therefore O
represents O
a O
more O
scalable O
data B-Method
- I-Method
driven I-Method
approach I-Method
. O

We O
distribute O
the O
standard O
layers O
of O
the O
model O
and O
the O
gating B-Method
network I-Method
according O
to O
conventional O
data B-Method
- I-Method
parallel I-Method
schemes I-Method
, O
but O
keep O
only O
one O
shared O
copy O
of O
each O
expert O
. O

; O
[ O
blue O
hbox O
, O
right O
of O
= O
c O
- O
3u O
, O
node O
distance=0.7 O
cm O
] O
( O
c O
- O
4u O
) O
; O
[ O
dots O
hbox O
, O
right O
of O
= O
c O
- O
4u O
, O
node O
distance=0.7 O
cm O
] O

Therefore O
we O
adopt O
a O
view O
- O
specific O
detection B-Task
adjustment O
to O
alleviate O
the O
offsets O
. O

The O
same O
encoder O
with O
the O
same O
embedding O
matrix O
is O
also O
used O
to O
obtain O
the O
question O
vector O
q O
from O
q. O

During O
our O
training O
, O
in O
parallel O
with O
our O
newly O
proposed O
hard B-Task
image I-Task
mining I-Task
, O
we O
also O
exploit O
the O
traditional O
hard B-Method
anchor I-Method
mining I-Method
method I-Method
to O
focus O
more O
on O
the O
hard O
and O
misclassificed O
anchors O
. O

Since O
there O
are O
virtually O
three O
output O
layers O
in O
FCN B-Method
- I-Method
8s I-Method
, O
we O
experiment O
with O
all O
the O
three O
feature O
layers O
correspondingly O
. O

Our O
proposed O
solution O
draws O
on O
two O
key O
observations O
. O

Despite O
the O
higher O
layers O
in O
deep B-Method
neural I-Method
network I-Method
can O
involve O
the O
spatial O
context O
information O
around O
the O
objects O
due O
to O
the O
large O
receptive O
field O
, O
Zhou O
have O
shown O
that O
the O
practical O
receptive O
field O
is O
actually O
much O
smaller O
than O
the O
theoretical O
one O
. O

, O
which O
can O
be O
defined O
similarly O
to O
the O
update O
gate O
function O
: O

saturates O
at O
around O
100 O
samples O
. O

As O
we O
stated O
before O
, O
supervisions O
to O
train O
the O
- B-Method
th I-Method
instance I-Method
classifier I-Method
are O
generated O
based O
on O
proposal O
scores O
and O
image O
label O
. O

This O
shows O
the O
effectiveness O
of O
using O
temporal O
information O
. O

The O
features O
extracted O
from O
the O
larger O
network B-Method
match O
or O
outperform O
the O
best O
prior O
result O
on O
all O
datasets O
. O

section O
: O
Introduction O

The O
intuition O
is O
that O
in O
order O
to O
detect O
faces O
with O
different O
sizes O
, O
different O
effective O
receptive O
fields O
are O
required O
. O

The O
fully B-Method
connected I-Method
layers I-Method
serve O
as O
classifiers B-Method
which O
take O
fixed O
- O
size O
inputs O
, O
thus O
require O
the O
width O
and O
height O
of O
input O
images O
to O
be O
fixed O
. O

This O
results O
in O
computing O
for O
all O
elements O
in O
the O
batch O
the O
loss O
with O
where O
is O
a O
time O
index O
for O
an O
experience O
sampled O
from O
the O
replay O
starting O
with O
state O
and O
action O
, O
and O
denotes O
parameters O
of O
the O
target O
network B-Method
dqn I-Method
, O
a O
slow O
moving O
copy O
of O
the O
online O
parameters O
. O

Objects O
of O
revolution O
are O
a O
special O
and O
simpler O
case O
: O
since O
their O
angle O
of O
symmetry O
is O
0 O

Exploiting O
the O
ability O
of O
sequence B-Method
- I-Method
to I-Method
- I-Method
sequence I-Method
networks I-Method
to O
take O
into O
account O
the O
events O
in O
the O
past O
, O
to O
predict O
temporally O
consistent O
3D O
poses O
. O

The O
aggregate O
pixels O
in O
all O
subsampled O
channels O
are O
then O
vectorized O
into O
a O
pixel O
look O
- O
up O
table O
. O

We O
compare O
to O
the O
closest O
most O
recent O
method O
, O
asymmetric B-Method
tritraining I-Method
[ O
reference O
] O
. O

3.2.1 O
through O
3.2.3 O
. O

The O
Selective B-Method
Search I-Method
( I-Method
SS I-Method
) I-Method
proposal I-Method
takes O
about O
1 O
- O
2 O
seconds O
per O
image O
on O
a O
CPU O
. O

As O
there O
is O
no O
mathematical O
solution O
to O
the O
problem O
described O
in O
Eqn O
. O

The O
ground O
- O
truth O
( O
including O
a O
mean O
and O
a O
standard O
deviation O
) O
is O
calculated O
from O
all O
the O
votes O
. O

Since O
then O
, O
hashing B-Method
methods I-Method
have O
also O
been O
advanced O
to O
reduce O
the O
number O
of O
parameters O
. O

We O
note O
that O
the O
regularization O
term O
of O
our O
objective O
is O
equivalent O
to O
minimizing O
the O
variance O
in O
the O
prediction O
function O
with O
different O
dropout O
masks O
as O
shown O
below O
( O
proof O
in O
the O
appendix O
) O
. O

and O
σ O
2 O
θ O
. O

Despite O
the O
progress O
that O
DNNs B-Method
achieve O
, O
there O
still O
are O
some O
problems O
. O

We O
can O
view O
the O
BiRNN B-Method
encoding O
of O
an O
item O
as O
representing O
the O
item O
together O
with O
a O
context O
of O
an O
infinite O
window O
around O
it O
. O

In O
contrast O
to O
prior O
arts O
, O
the O
semantic B-Task
segmentation I-Task
we O
study O
in O
this O
paper O
is O
a O
highly O
structured B-Task
prediction I-Task
problem I-Task
, O
for O
which O
domain B-Task
adaptation I-Task
is O
only O
sparsely O
explored O
in O
the O
literature O
. O

With O
the O
same O
number O
of O
regressors O
, O
the O
RF B-Method
- O
based O
algorithm O
can O
outperform O
or O
achieve O
comparable O
performance O
with O
A O
+ O
and O
its O
variants O
, O
in O
terms O
of O
accuracy B-Metric
but O
with O
less O
computational B-Metric
complexity I-Metric
. O

We O
argue O
that O
there O
is O
no O
speed O
or O
accuracy B-Metric
degradation I-Metric
for O
large B-Task
faces I-Task
, O
since O
inferring O
on O
a O
tiny O
image O
( O
with O
short O
side O
containing O
100 O
or O
300 O
pixels O
) O
is O
very O
fast O
, O
and O
the O
shrinked O
large O
face O
will O
still O
have O
enough O
information O
to O
be O
recognized O
. O

In O
addition O
, O
we O
also O
find O
that O
Eq O
. O

section O
: O
Experiment O
Details O

Deep B-Method
learning I-Method
is O
commonly O
used O
to O
either O
learn O
a O
person B-Method
’s I-Method
representation I-Method
or O
the O
distance B-Metric
metric I-Metric
. O

Given O
the O
word O
embeddings O
, O
this O
task O
is O
solved O
by O
finding O
the O
word O
whose O
embedding O
is O
closest O
to O
the O
vector O
in O
terms O
of O
cosine O
proximity O
, O
i.e. O
, O
. O

The O
proposed O
PDC B-Method
model O
learns O
the O
global B-Method
representation I-Method
depicting O
the O
whole O
body O
and O
local B-Method
representations I-Method
depicting O
body O
parts O
simultaneously O
. O

/ O
S O

For O
example O
, O
would O
be O
a O
- O
dimensional O
vector O
of O
features O
extracted O
from O
the O
upper O
left O
corner O
. O

– O
( O
ap O
- O
8 O
) O
; O
[ O
tbox O
, O
above O
of O
= O
ap O
- O
4 O
, O
node O
distance=0.9cm*1.6 O
, O
xshift=1 O

In O
a O
rough O
form O
, O
the O
diagonal O
weight O
matrix O
is O
given O
as O
follows O
: O

SNM B-Method
estimation I-Method
is O
closely O
related O
to O
all O
n B-Method
- I-Method
gram I-Method
LM I-Method
smoothing I-Method
techniques I-Method
that O
rely O
on O
mixing O
relative O
frequencies O
at O
various O
orders O
. O

First O
and O
second O
row O
: O
since O
MNC B-Method
predicts O
instances O
independently O
, O
it O
is O
prone O
to O
predicting O
multiple O
instances O
for O
a O
single O
person O
. O

Table O
[ O
reference O
] O
shows O
the O
results O
. O

Interestingly O
, O
when O
adding O
external O
word O
embeddings O
the O
accuracy B-Metric
of O
the O
graph B-Method
- I-Method
based I-Method
parser O
degrades O
. O

The O
acoustic B-Material
training I-Material
data I-Material
is O
comprised O
by O
LDC B-Material
corpora I-Material
97S62 O
, O
2004S13 O
, O
2005S13 O
, O
2004S11 O
and O
2004S09 O
; O
see O
for O
a O
full O
description O
. O

After O
bounding B-Method
box I-Method
regression I-Method
, O
our O
5 B-Metric
- I-Metric
scale I-Metric
result I-Metric
( O
59.2 O
% O
) O
is O
0.7 O
% O
better O
than O
R B-Method
- I-Method
CNN I-Method
( O
58.5 O
% O
) O
, O
and O
our O
1 B-Metric
- I-Metric
scale I-Metric
result I-Metric
( O
58.0 O
% O
) O
is O
0.5 O
% O
worse O
. O

Hence O
, O
one O
may O
force O
a O
given O
neural B-Method
network I-Method
to O
have O
the O
same O
predictions O
for O
different O
augmentations O
. O

Second O
, O
in O
the O
case O
of O
TD B-Method
/ O
BU B-Method
model O
the O
graph O
is O
sparse O
and O
a O
large O
portion O
of O
the O
computation O
is O
performed O
by O
the O
feed B-Method
- I-Method
forward I-Method
CNN I-Method
introduced O
in O
Sec O
. O

Obviously O
, O
a O
query O
entry O
falling O
into O
a O
bigger O
cluster O
and O
closer O
to O
the O
center O
of O
the O
belonging O
cluster O
achieves O
a O
larger O
weight O
. O

For O
example O
, O
the O
work O
of O
chen2014fast O
uses O
18 O
different O
elements O
in O
its O
feature O
function O
, O
while O
the O
work O
of O
pei2015effective O
uses O
21 O
different O
elements O
. O

section O
: O
CONCLUSIONS O

Therefore O
, O
we O
set O
to O
for O
the O
other O
experiments O
. O

Across O
the O
system O
, O
we O
take O
advantage O
of O
this O
by O
batching O
all O
communications O
with O
the O
centralized B-Method
replay I-Method
, O
increasing O
the O
efficiency O
and O
throughput B-Metric
at O
the O
cost O
of O
some O
latency O
. O

We O
study O
the O
adaptation B-Metric
capability I-Metric
of O
DANN B-Method
by O
comparing O
it O
to O
the O
standard O
neural B-Method
network I-Method
( O
NN B-Method
) O
. O

The O
probability O
values O
should O
have O
difference O
among O
all O
possible O
labels O
associated O
with O
an O
image O
. O

For O
the O
“ O
word B-Method
dropout I-Method
” I-Method
model I-Method
, O
we O
randomly O
replace O
words O
in O
the O
input O
sentence O
with O
a O
REMOVED O
token O
with O
probability O
0.1 O
( O
this O
value O
worked O
well O
on O
the O
dev B-Metric
sets O
) O
. O

Independent O
samples O
of O
and O
correspond O
to O
samples O
from O
the O
data O
distribution O
and O
the O
categorical O
distribution O
. O

– O
( O
c O
- O
3 O
) O
; O

We O
further O
define O
loss O
on O
surface O
normal O
to O
characterize O
high O
order O
properties O
: O
where O
is O
the O
closest O
vertex O
for O
that O
is O
found O
when O
calculating O
the O
chamfer O
loss O
, O
is O
the O
neighboring O
pixel O
of O
, O
is O
the O
inner O
product O
of O
two O
vectors O
, O
and O
is O
the O
observed O
surface O
normal O
from O
ground O
truth O
. O

Under O
this O
" O
fully O
- O
transductive O
" O
setting O
, O
our O
method O
is O
able O
to O
improve O
previously O
- O
reported O
state O
- O
of O
- O
the O
- O
art O
accuracy B-Metric
for O
unsupervised B-Task
adaptation I-Task
very O
considerably O
( O
Table O
3 O
) O
, O
especially O
in O
the O
most O
challenging O
Amazon O
→ O

We O
address O
this O
problem O
by O
using O
a O
multi B-Method
- I-Method
stream I-Method
multi I-Method
- I-Method
scale I-Method
architecture I-Method
. O

In O
fact O
, O
in O
many O
cases O
the O
noise O
will O
actualy O
result O
in O
a O
lower O
objective B-Metric
function I-Metric
value I-Metric
. O

Interestingly O
, O
when O
we O
interpolate O
the O
two O
models O
, O
we O
have O
an O
additional O
gain O
of O
20 O
% O
, O
and O
as O
far O
as O
we O
know O
, O
the O
perplexity B-Metric
of O
41.3 O
is O
already O
the O
best O
ever O
reported O
on O
this O
database O
, O
beating O
the O
previous O
best O
by O
6 O
% O
[ O
reference O
] O
. O

In O
the O
evaluation B-Task
phase I-Task
, O
the O
probability O
of O
a O
query O
sample O
x O
belonging O
to O
class O
k O
is O
given O
by O
averaging O
all O
the O
trees O
, O
or O
by O
other O
methods O
. O

However O
, O
we O
do O
not O
use O
the O
multi B-Method
- I-Method
scale I-Method
fusion I-Method
strategy I-Method
due O
to O
the O
memory O
issue O
. O

The O
weights O
of O
the O
LSTM B-Method
units O
are O
initialized O
by O
Xavier B-Method
uniform I-Method
initializer I-Method
. O

Our O
model O
does O
not O
require O
any O
prior O
knowledge O
of O
the O
3D O
shape O
, O
and O
always O
deform O
from O
an O
initial O
ellipsoid O
with O
average O
size O
placed O
at O
the O
common O
location O
in O
the O
camera O
coordinate O
. O

In O
those O
cases O
, O
the O
vector B-Method
representation I-Method
of O
a O
word O
is O
a O
concatenation O
of O
its O
randomly B-Method
- I-Method
initialized I-Method
vector I-Method
embedding I-Method
with O
its O
pre O
- O
trained O
word O
vector O
. O

section O
: O
Handling O
Objects O
with O
an O
Axis O
of O
Symmetry O

Others O
model O
3D B-Task
pose I-Task
as O
a O
sparse B-Method
linear I-Method
combination I-Method
of O
an O
over O
- O
complete O
dictionary O
of O
basis O
poses O
. O

Multi B-Method
- I-Method
view I-Method
learning I-Method
on O
data O
where O
features O
can O
be O
separated O
into O
distinct O
subsets O
has O
been O
well O
studied O
Xu2013ASO O
. O

We O
refer O
to O
the O
subset O
of O
this O
data O
set O
that O
includes O
the O
first O
pair O
of O
cameras O
only O
as O
CUHK O
/ O
p1 O
( O
as O
most O
papers O
use O
this O
subset O
) O
. O

In O
the O
past O
CTC B-Method
networks O
have O
been O
decoded O
using O
either O
a O
form O
of O
best B-Method
- I-Method
first I-Method
decoding I-Method
known O
as O
prefix B-Method
search I-Method
, O
or O
by O
simply O
taking O
the O
most O
active O
output O
at O
every O
timestep O
. O

The O
d O
- O
steps O
in O
all O
three O
training O
strategies O
described O
above O
is O
set O
to O
1 O
, O
which O
means O
we O
only O
generate O
one O
set O
of O
negative O
examples O
with O
the O
same O
number O
as O
the O
given O
dataset O
, O
and O
then O
train O
the O
discriminator O
on O
it O
for O
various O
epochs O
. O

In O
adversarial B-Method
adaptive I-Method
methods I-Method
, O
the O
main O
goal O
is O
to O
regularize O
the O
learning O
of O
the O
source O
and O
target O
mappings O
, O
and O
, O
so O
as O
to O
minimize O
the O
distance O
between O
the O
empirical O
source O
and O
target O
mapping O
distributions O
: O
and O
. O

subsection O
: O
Model O
Setup O

can O
not O
; O
2 O
) O
SPP B-Method
uses O
multi O
- O
level O
spatial O
bins O
, O
while O
the O
sliding B-Method
window I-Method
pooling I-Method
uses O
only O
a O
single O
window O
size O
. O

It O
contains O
16097 O
images O
, O
and O
is O
evaluated O
by O
WIDER B-Material
FACE I-Material
author I-Material
team I-Material
. O

This O
makes O
sense O
since O
high O
semantic B-Metric
matching I-Metric
scores I-Metric
are O
positive O
indicators O
on O
answer B-Metric
correctness I-Metric
, O
whereas O
low B-Metric
semantic I-Metric
matching I-Metric
scores I-Metric
indicate O
that O
the O
candidate O
answer O
sentences O
contain O
irrelevant O
terms O
. O

The O
output O
of O
different O
levels O
in O
the O
pyramid B-Method
pooling I-Method
module I-Method
contains O
the O
feature O
map O
with O
varied O
sizes O
. O

Traditional O
methods O
such O
as O
Total B-Method
variation I-Method
, O
BM3D B-Method
algorithm I-Method
and O
dictionary B-Method
learning I-Method
based I-Method
methods I-Method
have O
shown O
very O
good O
performance O
on O
image B-Task
restoration I-Task
topics I-Task
such O
as O
image O
denoising B-Task
and O
super B-Task
- I-Task
resolution I-Task
. O

Such O
an O
instance B-Method
- I-Method
agnostic I-Method
setting I-Method
severely O
deviates O
from O
reality O
. O

Criteo B-Material
Dataset I-Material
. O

This O
solution O
maintains O
the O
complete O
content O
, O
but O
introduces O
distortion O
. O

We O
use O
the O
following O
implementation O
to O
handle O
all O
bins O
when O
applying O
the O
network O
. O

Â O
For O
larger O
datasets O
such O
as O
Imagenet B-Material
, O
the O
recent O
trend O
has O
been O
to O
increase O
the O
number O
of O
layers O
and O
layer O
size O
, O
while O
using O
dropout B-Method
to O
address O
the O
problem O
of O
overfitting B-Task
. O

The O
fact O
that O
these O
simple O
, O
cheap B-Method
algorithms I-Method
are O
able O
to O
generate O
misclassified O
examples O
serves O
as O
evidence O
in O
favor O
of O
our O
interpretation O
of O
adversarial O
examples O
as O
a O
result O
of O
linearity O
. O

The O
apparition O
of O
inexpensive O
3D B-Method
cameras I-Method
favored O
the O
development O
of O
methods O
suitable O
for O
untextured O
objects O
: O
[ O
reference O
][ O
reference O
] O
rely O
on O
depth O
data O
only O
and O
use O
votes O
from O
pairs O
of O
3D O
points O
and O
their O
normals O
to O
detect O
3D O
objects O
. O

subsection O
: O
Identification B-Method
Models I-Method

First O
, O
we O
combined O
it O
with O
a O
variant O
of O
DQN B-Method
dqn O
with O
some O
of O
the O
components O
of O
Rainbow B-Method
rainbow I-Method
. O

If O
we O
construct O
left O
- O
branching O
trees O
in O
a O
bottom O
- O
up O
fashion O
, O
the O
model O
acts O
just O
like O
sequential O
LSTM B-Method
. O

SRCNN B-Method
and O
VSRnet B-Method
upsample O
LR O
images O
before O
attempting O
to O
super O
- O
resolve O
them O
, O
which O
considerably O
increases O
the O
required O
number O
of O
operations O
. O

3.4 O
of O
the O
paper O
on O
our O
Multi B-Material
- I-Material
Person I-Material
Video I-Material
dataset I-Material
. O

Carreira O
et O
al O
. O

For O
problems O
where O
is O
unknown O
, O
we O
will O
show O
that O
a O
reasonably O
chosen O
also O
works O
well O
in O
DLDL B-Method
. O

( O
6 O
) O
can O
be O
seen O
as O
a O
regularizer B-Method
enforcing O
all O
h O
( O
Tαxi O
) O
to O
be O
close O
to O
their O
average O
value O
, O
i.e. O
, O
the O
feature B-Method
representation I-Method
is O
sought O
to O
be O
approximately O
invariant O
to O
the O
transformations O
Tα O
. O

Also O
we O
are O
indebted O
to O
the O
DistBelief O
team O
for O
their O
support O
especially O
to O
Rajat O
Monga O
, O
Jon O
Shlens O
, O
Alex O
Krizhevsky O
, O
Jeff O
Dean O
, O
Ilya O
Sutskever O
and O
Andrea O
Frome O
. O

[ O
reference O
] O
to O
show O
the O
contribution O
of O
each O
component O
in O
our O
system O
. O

Experimental O
results O
are O
presented O
in O
Table O
[ O
reference O
] O
and O
Table O
[ O
reference O
] O
. O

Activation O
functions O
are O
defined O
as O
usual O
, O
but O
are O
restricted O
to O
the O
set O
of O
active O
sites O
. O

Note O
that O
the O
state O
- O
of O
- O
the O
- O
art O
method O
uses O
a O
different O
baseline O
model O
, O
and O
we O
present O
it O
as O
a O
reference O
to O
analyze O
how O
much O
the O
proposed O
algorithm O
can O
improve O
. O

Out O
of O
the O
15 O
models O
built O
, O
only O
models O
given O
non O
- O
zero O
weight O
in O
the O
final O
system O
combination O
are O
shown O
. O

Finally O
we O
conclude O
in O
Section O
[ O
reference O
] O
. O

where O
the O
parameter O
balances O
the O
sparsity O
of O
the O
solution O
and O
the O
fidelity O
of O
the O
approximation O
to O
. O

In O
addition O
, O
the O
prediction O
of O
models O
with O
dropout O
generally O
vary O
with O
different O
dropout O
mask O
. O

Over O
the O
years O
, O
different O
techniques O
have O
been O
used O
to O
address O
the O
problem O
of O
3D B-Task
pose I-Task
estimation O
. O

[ O
reference O
] O
, O
the O
pillow O
has O
similar O
appearance O
with O
the O
sheet O
. O

it O
seems O
like O
some O
cameras O
are O
placed O
on O
the O
buildings O
that O
are O
significantly O
higher O
than O
a O
bus O
. O

[ O
red O
arrow O
, O
shorten O
¿ O
= O
1pt O
, O
shorten O
¡ O
= O
1pt O
] O
( O
c O
- O
1 O
) O

The O
words O
are O
projected O
to O
corresponding O
word B-Method
representations I-Method
: O
where O
. O

In O
the O
detection B-Method
algorithm I-Method
( O
and O
multi B-Task
- I-Task
view I-Task
testing I-Task
on I-Task
feature I-Task
maps I-Task
) O
, O
a O
window O
is O
given O
in O
the O
image O
domain O
, O
and O
we O
use O
it O
to O
crop O
the O
convolutional O
feature O
maps O
( O
, O
conv O
) O
which O
have O
been O
sub O
- O
sampled O
several O
times O
. O

We O
elaborately O
conduct O
many O
experiments O
on O
the O
challenging O
PASCAL B-Material
VOC I-Material
, O
ImageNet B-Material
detection I-Material
, O
and O
MS B-Material
- I-Material
COCO I-Material
datasets I-Material
to O
confirm O
the O
effectiveness O
of O
our O
method O
. O

We O
evaluate O
it O
in O
this O
section O
on O
three O
different O
datasets O
, O
including O
ImageNet B-Task
scene I-Task
parsing I-Task
challenge I-Task
2016 I-Task
, O
PASCAL B-Task
VOC I-Task
2012 I-Task
semantic I-Task
segmentation I-Task
and O
urban O
scene O
understanding O
dataset O
Cityscapes B-Material
. O

G O
a O
) O
, O
and O
the O
expert B-Method
networks I-Method
by O
( O
E O
0 O
, O
0 O
, O
E O
0 O
, O
1 O
.. O

al O
. O

Moreover O
, O
we O
would O
like O
to O
explore O
possible O
strategies O
to O
exploit O
monolingual O
training O
data O
in O
addition O
to O
parallel O
corpora O
, O
such O
as O
using O
pre O
- O
trained O
word B-Method
embeddings I-Method
, O
backtranslation O
sennrich2016improving O
, O
edunov2018understanding O
, O
or O
other O
ideas O
from O
unsupervised B-Method
machine I-Method
translation I-Method
Artetxe:2018:emnlp_unsupmt O
, O
Lample:2018:emnlp_unsupmt O
. O

Recently O
, O
great O
success O
has O
been O
achieved O
for O
3d B-Task
shape I-Task
generation I-Task
from O
a O
single O
color O
image O
using O
deep B-Method
learning I-Method
techniques I-Method
. O

In O
our O
experiments O
with O
recognizing B-Task
handwritten I-Task
digits I-Task
and I-Task
3D I-Task
shapes I-Task
, O
networks O
using O
SC B-Method
and O
VSC B-Method
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
whilst O
reducing O
the O
computation O
and O
memory B-Metric
requirements I-Metric
by O
. O

These O
results O
strongly O
indicate O
that O
a O
discriminative B-Method
objective I-Method
is O
superior O
to O
objectives O
previously O
used O
for O
unsupervised B-Method
feature I-Method
learning I-Method
. O

Apart O
from O
those O
differences O
, O
the O
architectures O
are O
identical O
. O

Recently O
, O
methods O
based O
on O
convolutional B-Method
neural I-Method
networks I-Method
( O
CNNs B-Method
) O
have O
achieved O
significant O
progress O
in O
semantic B-Task
segmentation I-Task
with O
applications O
for O
autonomous B-Task
driving I-Task
and I-Task
image I-Task
editing I-Task
. O

Still O
, O
one O
prescription O
that O
was O
verified O
to O
work O
very O
well O
after O
the O
competition O
includes O
sampling O
of O
various O
sized O
patches O
of O
the O
image O
whose O
size O
is O
distributed O
evenly O
between O
8 O
% O
and O
100 O
% O
of O
the O
image O
area O
and O
whose O
aspect O
ratio O
is O
chosen O
randomly O
between O
and O
. O

In O
fig O
: O
slow B-Method
- I-Method
fusion I-Method
we O
show O
a O
slow B-Method
fusion I-Method
network I-Method
where O
and O
the O
rate B-Metric
of I-Metric
fusion I-Metric
is O
defined O
by O
for O
or O
otherwise O
, O
meaning O
that O
at O
each O
layer O
only O
two O
consecutive O
frames O
or O
filter O
activations O
are O
merged O
until O
the O
network O
’s O
temporal O
depth O
shrinks O
to O
. O

After O
individual O
sentence B-Method
models I-Method
, O
we O
design O
a O
sentence B-Method
matching I-Method
layer I-Method
to O
aggregate O
information O
. O

if O
they O
are O
paraphrases O
( O
i.e. O
if O
they O
belong O
to O
the O
same O
paraphrase O
cluster O
) O
, O
and O
make O
them O
different O
otherwise O
. O

First O
, O
we O
pad O
the O
input O
with O
on O
each O
side O
, O
so O
that O
the O
output O
will O
have O
the O
same O
size O
as O
the O
input O
. O

( O
c O
) O
shows O
our O
results O
using O
multi B-Method
- I-Method
size I-Method
training I-Method
. O

Our O
sentence B-Method
embeddings I-Method
are O
also O
strong O
at O
parallel B-Task
corpus I-Task
mining I-Task
, O
establishing O
a O
new O
state O
- O
of O
- O
the O
- O
art O
in O
the O
BUCC B-Material
shared O
task O
for O
3 O
of O
its O
4 O
language O
pairs O
. O

This O
additional O
loss O
encourages O
all O
experts O
to O
have O
equal O
importance O
. O

The O
quantitative O
comparison O
on O
the O
NTIRE O
SR B-Task
2017 O
challenge O
is O
shown O
in O
Table O
[ O
reference O
] O
. O

This O
paper O
presented O
a O
novel O
network B-Method
embedding I-Method
model I-Method
called O
the O
“ O
LINE B-Method
, O
” O
which O
can O
easily O
scale O
up O
to O
networks O
with O
millions O
of O
vertices O
and O
billions O
of O
edges O
. O

The O
standard O
deviation O
, O
however O
, O
is O
provided O
in O
ChaLearn B-Material
but O
not O
in O
Morph B-Material
. O

They O
are O
higher O
than O
the O
rank1 B-Metric
accuracy O
and O
mAP B-Metric
of I-Metric
PIE I-Metric
, O
which O
performs O
best O
among O
the O
compared O
works O
. O

In O
this O
context O
, O
we O
consider O
assigning O
a O
probability O
greater O
than O
0.5 O
to O
any O
class O
to O
be O
an O
error B-Metric
. O

The O
framework O
allows O
for O
a O
variety O
of O
graphs O
and O
connectivity O
patterns O
. O

The O
R B-Method
- I-Method
CNN I-Method
and I-Method
OverFeat I-Method
detectors I-Method
lead O
this O
wave O
with O
impressive O
results O
on O
PASCAL B-Material
VOC I-Material
and O
ImageNet B-Task
detection I-Task
. O

Therefore O
, O
we O
can O
rotate O
and O
mirror O
flip O
the O
kernels O
and O
perform O
forward O
multiple O
times O
, O
and O
then O
average O
the O
output O
to O
get O
a O
more O
smooth O
image O
. O

The O
question B-Task
attention I-Task
network O
uses O
the O
gating O
function O
to O
control O
the O
output O
of O
each O
network O
in O
this O
process O
. O

– O
( O
ai O
- O
15u O
) O
; O
arrow O
, O

We O
then O
compute O
the O
loss O
for O
each O
category O
and O
back O
- O
propagate O
the O
error O
gradients O
to O
the O
earlier O
layers O
. O

Word O
embeddings O
were O
pretrained O
ourselves O
by O
word2vec O
on O
the O
English O
Wikipedia O
corpus O
and O
fined O
tuned O
during O
training O
as O
a O
part O
of O
model O
parameters O
. O

CNN B-Method
: O

In O
Section O
2 O
, O
the O
detailed O
realization O
of O
the O
generative B-Method
model I-Method
and O
the O
discriminative B-Method
model I-Method
is O
discussed O
, O
including O
the O
model O
parameter O
settings O
. O

Does O
this O
padding O
character O
influence O
the O
performance O
of O
the O
NTI B-Method
models O
? O

QRN B-Method
is O
inspired O
by O
RNN B-Method
- O
based O
models O
with O
gating B-Method
mechanism I-Method
, O
such O
as O
LSTM B-Method
[ O
reference O
] O
and O
GRU B-Method
[ O
reference O
] O
. O

Multi B-Method
- I-Method
Task I-Method
Learning O
. O

We O
show O
that O
the O
SPP B-Method
- O
nets O
can O
boost O
various O
networks O
that O
are O
deeper O
and O
larger O
( O
Sec O
. O

A O
crucial O
aspect O
which O
we O
discuss O
in O
detail O
in O
later O
sections O
is O
the O
size O
of O
our O
models O
. O

section O
: O
Pairwise B-Method
Term I-Method

An O
information B-Task
network I-Task
is O
defined O
as O
= O
G O
( O
V O
, O
E O
) O
, O
where O
V O
is O
the O
set O
of O
vertices O
, O
each O
representing O
a O
data O
object O
and O
E O
is O
the O
set O
of O
edges O
between O
the O
vertices O
, O
each O
representing O
a O
relationship O
between O
two O
data O
objects O
. O

We O
implement O
our O
algorithm O
with O
Tensorflow B-Method
framework I-Method
. O

Put O
differently O
, O
learning B-Method
filters I-Method
from O
all O
the O
data O
assumes O
that O
the O
decision O
boundary O
is O
defined O
by O
a O
single O
distribution O
( O
like O
in O
Linear B-Method
Discriminant I-Method
Analysis I-Method
) O
, O
while O
we O
might O
want O
to O
define O
it O
based O
on O
the O
relation O
between O
the O
background O
distribution O
and O
the O
foreground O
distribution O
( O
like O
Fisher B-Method
’s I-Method
Discriminant I-Method
Analysis I-Method
) O
. O

Words O
outside O
of O
the O
vocabulary O
were O
mapped O
to O
< O
UNK O
> O
token O
, O
also O
part O
of O
the O
vocabulary O

Table O
[ O
reference O
] O
presents O
the O
results O
of O
our O
model O
and O
the O
previous O
models O
for O
the O
task O
. O

We O
also O
achieve O
very O
competitive O
results O
in O
cross B-Task
- I-Task
lingual I-Task
document I-Task
classification I-Task
( O
MLDoc B-Material
dataset I-Material
) O
. O

To O
differentiate O
high O
- O
resolution O
real O
and O
synthesized O
images O
, O
the O
discriminator B-Method
needs O
to O
have O
a O
large O
receptive O
field O
. O

Typically O
, O
we O
divide O
the O
out O
- O
of O
- O
plane O
rotation O
¡ O
° O
yaw¡± O
into O
different O
views O
and O
let O
the O
classifier B-Method
itself O
tolerate O
the O
pose O
variance O
in O
the O
other O
two O
types O
of O
rotations O
. O

The O
large O
capacity O
of O
the O
network O
is O
the O
key O
to O
this O
success O
. O

The O
fraternal B-Method
dropout I-Method
method I-Method
is O
general O
and O
may O
be O
applied O
in O
feed B-Method
- I-Method
forward I-Method
architectures I-Method
( O
as O
shown O
in O
Subsection O
[ O
reference O
] O
for O
CIFAR B-Material
- I-Material
10 I-Material
semisupervised I-Material
example I-Material
) O
. O

In O
this O
method O
, O
the O
hidden O
code O
is O
encouraged O
to O
learn O
the O
global O
information O
that O
is O
location O
- O
invariant O
( O
the O
what O
information O
and O
not O
the O
where O
information O
) O
such O
as O
the O
class O
label O
information O
. O

Several O
papers O
, O
however O
, O
observed O
that O
the O
performance O
of O
the O
resulting O
re B-Method
- I-Method
identification I-Method
systems I-Method
drops O
very O
considerably O
when O
descriptors O
trained O
on O
one O
data O
set O
and O
tested O
on O
another O
. O

( O
KN B-Method
) O
models O
. O

We O
select O
23 O
, O
409 O
faces O
to O
ensure O
pitch O
and O
yaw O
angles O
within O
. O

In O
order O
to O
better O
understand O
and O
visualize O
the O
effect O
of O
the O
adversarial B-Method
training O
on O
shaping O
the O
hidden O
code O
distribution O
, O
we O
train O
a O
PixelGAN O
autoencoder B-Method
on O
the O
first O
three O
digits O
of O
MNIST B-Material
( O
18000 O
training O
and O
3000 O
test O
points O
) O
and O
choose O
the O
number O
of O
clusters O
to O
be O
3 O
. O

Fig O
. O

subsection O
: O
Language B-Task
Models I-Task

section O
: O
Recurrent B-Method
Neural I-Method
Networks I-Method

For O
the O
evaluation O
of O
performance O
of O
the O
proposed O
method O
on O
the O
NLI B-Task
task I-Task
, O
SNLI B-Material
and O
MultiNLI B-Material
datasets I-Material
are O
used O
. O

Finally O
, O
our O
system O
combines O
these O
individual O
components O
into O
a O
single O
, O
jointly B-Method
optimized I-Method
model I-Method
. O

in O
order O
to O
increase O
the O
representational B-Method
power I-Method
of O
neural B-Method
networks I-Method
. O

The O
final O
row O
in O
Table O
[ O
reference O
] O
shows O
the O
mAPs O
after O
adding O
a O
new O
set O
of O
cascades B-Method
trained O
from O
COCO B-Material
images I-Material
. O

Consider O
the O
ground O
- O
truth O
distribution O
over O
labels O
for O
this O
training O
example O
, O
normalized O
so O
that O
. O

¡ O
= O
- O
2pt O
] O

Then O
we O
propose O
six O
rectangles O
to O
cover O
six O
different O
parts O
of O
human O
body O
, O
including O
the O
head O
region O
, O
the O
upper O
body O
, O
two O
arms O
and O
two O
legs O
. O

On O
the O
testing O
stage O
, O
we O
resize O
an O
image O
so O
where O
represents O
a O
predefined O
scale O
( O
like O
256 O
) O
. O

A O
more O
principled O
way O
to O
combine O
the O
two O
proximity O
is O
to O
jointly O
train O
the O
objective O
function O
( O
[ O
reference O
] O
) O
and O
( O
[ O
reference O
] O
) O
, O
which O
we O
leave O
as O
future O
work O
. O

The O
scales O
play O
important O
roles O
in O
traditional O
methods O
, O
, O
the O
SIFT O
vectors O
are O
often O
extracted O
at O
multiple O
scales O
( O
determined O
by O
the O
sizes O
of O
the O
patches O
and O
Gaussian B-Method
filters I-Method
) O
. O

In O
particular O
, O
in O
contrast O
to O
simply O
reducing O
the O
replay O
capacity O
, O
duplicating O
each O
data O
point O
means O
that O
the O
computational O
demands O
on O
the O
replay B-Method
server I-Method
in O
these O
runs O
are O
the O
same O
as O
when O
we O
use O
the O
corresponding O
number O
of O
real O
actors O
. O

With O
more O
stages O
, O
the O
effective O
receptive O
field O
is O
even O
larger O
. O

In O
addition O
, O
the O
CRF B-Method
further O
improve O
performance O
of O
DLDL B-Method
- I-Method
8s I-Method
, O
offering O
a O
2.6 O
% O
absolute O
increase O
in O
mean B-Metric
IU I-Metric
both O
on O
VOC2011 O
and O
VOC2012 O
. O

And O
, O
different O
people O
may O
have O
different O
guesses O
towards O
the O
same O
face O
. O

We O
use O
the O
different O
notation O
for O
the O
background O
cluster O
because O
background O
proposals O
are O
scattered O
in O
each O
image O
, O
and O
thus O
it O
is O
hard O
to O
determine O
a O
cluster O
center O
and O
accordingly O
a O
cluster O
score O
. O

We O
stick O
to O
the O
simplest O
parsers B-Method
in O
each O
category O
– O
greedy B-Method
inference I-Method
for O
the O
transition B-Method
- I-Method
based I-Method
architecture I-Method
, O
and O
a O
first B-Method
- I-Method
order I-Method
, I-Method
arc I-Method
- I-Method
factored I-Method
model I-Method
for O
the O
graph B-Method
- I-Method
based I-Method
architecture O
. O

Recently O
, O
recurrent B-Method
neural I-Method
networks I-Method
( O
RNNs B-Method
) O
with O
long B-Method
short I-Method
- I-Method
term I-Method
memory I-Method
( O
LSTM B-Method
) O
cells O
have O
shown O
excellent O
performance O
ranging O
from O
natural B-Task
language I-Task
generation I-Task
to O
handwriting B-Task
generation I-Task
. O

Subgraph O
of O
a O
candidate O
answer O
a O
( O
here O
K. O
Preston O
) O

Taking O
random B-Method
- I-Method
forest I-Method
- I-Method
based I-Method
classification I-Method
as O
an O
example O
, O
training O
a O
single O
decision B-Method
tree I-Method
involves O
recursively O
splitting O
each O
node O
, O
such O
that O
the O
training O
data O
in O
each O
newly O
created O
child O
node O
is O
clustered O
according O
to O
their O
corresponding O
class O
labels O
, O
so O
the O
purity O
at O
each O
node O
is O
increasing O
along O
a O
tree O
. O

Thus O
, O
given O
the O
trained O
network O
( O
NN B-Method
or O
DANN B-Method
) O
, O
every O
point O
from O
S O
and O
T O
is O
mapped O
into O
a O
15 O
- O
dimensional O
feature O
space O
through O
the O
hidden O
layer O
, O
and O
projected O
back O
into O
a O
two O
- O
dimensional O
plane O
by O
the O
PCA B-Method
transformation I-Method
. O

The O
mapping O
from O
input O
images O
x O
to O
a O
feature B-Method
representation I-Method
g I-Method
( I-Method
x I-Method
) O
should O
then O
satisfy O
two O
requirements O
: O
( O
1 O
) O
there O
must O
be O
at O
least O
one O
feature O
that O
is O
similar O
for O
images O
of O
the O
same O
category O
y O
( O
invariance O
) O
; O
( O
2 O
) O
there O
must O
be O
at O
least O
one O
feature O
that O
is O
sufficiently O
different O
for O
images O
of O
different O
categories O
( O
ability O
to O
discriminate O
) O
. O

Similarly O
in O
MultiNLI B-Material
, O
our O
models O
match O
the O
accuracy B-Metric
of O
state O
- O
of O
- O
the O
- O
art O
models O
in O
both O
in B-Task
- I-Task
domain I-Task
( O
matched O
) O
and O
cross O
- O
domain O
( O
mismatched O
) O
test O
sets O
. O

Unlike O
many O
multi B-Task
- I-Task
task I-Task
learning I-Task
applications I-Task
, O
in O
our O
method O
the O
sub O
- O
nets O
depend O
on O
each O
other O
, O
forming O
a O
causal O
nest O
by O
dynamically O
boosting O
each O
other O
through O
an O
adversarial B-Method
strategy I-Method
( O
See O
Fig O
. O

We O
implement O
our O
approach O
with O
GTX B-Method
TITAN I-Method
X I-Method
GPU I-Method
, O
Intel O
i7 O
CPU O
, O
and O
128 O
GB O
memory O
. O

The O
second O
method O
is O
used O
for O
non B-Method
- I-Method
SVMs I-Method
and O
has O
been O
applied O
to O
a O
variety O
of O
models O
including O
shallow B-Method
neural I-Method
networks I-Method
and O
boosted B-Method
decision I-Method
trees I-Method
. O

For O
the O
HumanEva B-Material
dataset I-Material
, O
we O
report O
results O
on O
each O
subject O
and O
action O
separately O
after O
performing O
rigid B-Task
alignment I-Task
with O
the O
ground O
truth O
data O
, O
following O
the O
protocol O
used O
by O
the O
previous O
methods O
. O

BU B-Method
- O
sparse O
also O
achieves O
the O
best O
overall O
score O
of O
compared O
to O
by O
TD B-Method
/ O
BU B-Method
. O

In O
our O
experiments O
, O
we O
fed O
the O
node O
embeddings O
to O
a O
fully B-Method
- I-Method
connected I-Method
neural I-Method
network I-Method
and O
applied O
each O
pooling B-Method
method I-Method
element O
- O
wise O
. O

To O
solve O
this O
bottleneck O
, O
we O
borrow O
the O
concept O
of O
channel O
features O
to O
the O
face O
detection B-Task
domain O
, O
which O
extends O
the O
image O
channel O
to O
diverse O
types O
like O
gradient O
magnitude O
and O
oriented O
gradient O
histograms O
and O
therefore O
encodes O
rich O
information O
in O
a O
simple O
form O
. O

In O
addition O
, O
gradient O
clipping O
with O
threshold O
was O
found O
to O
be O
useful O
to O
stabilize O
the O
training B-Task
. O

section O
: O
Sparse B-Method
Non I-Method
- I-Method
negative I-Method
Modeling I-Method
for O
Skip B-Task
n I-Task
- I-Task
grams I-Task

( O
m O
- O
10 O
) O

Then O
, O
we O
adopt O
the O
same O
training B-Method
strategy I-Method
as O
described O
above O
. O

Current O
detection B-Method
systems I-Method
repurpose O
classifiers B-Method
to O
perform O
detection B-Task
. O

The O
goal O
of O
DLDL B-Method
is O
to O
directly O
learn O
a O
conditional O
probability O
mass O
function O
from O
, O
where O
is O
the O
parameters O
in O
the O
framework O
. O

Testset O
contains O
natural O
images O
with O
faces O
that O
vary O
a O
lot O
in O
pose O
, O
appearance O
and O
illumination O
. O

Additionally O
, O
fraternal B-Method
dropout I-Method
is O
more O
robust O
to O
different O
hyper O
- O
parameters O
choice O
( O
more O
runs O
performing O
better O
than O
the O
baseline O
and O
better O
average O
for O
top O
performing O
runs O
) O
. O

This O
also O
makes O
training O
the O
coarse B-Task
- I-Task
to I-Task
- I-Task
fine I-Task
generator I-Task
easier O
, O
since O
extending O
a O
low B-Method
- I-Method
resolution I-Method
model I-Method
to O
a O
higher O
resolution O
only O
requires O
adding O
a O
discriminator B-Method
at O
the O
finest O
level O
, O
rather O
than O
retraining O
from O
scratch O
. O

Various O
problems O
e.g. O
sequence B-Task
labeling I-Task
, O
sequence B-Task
generation I-Task
, O
and O
language B-Task
modeling I-Task
might O
benefit O
from O
sophisticated O
modulation O
on O
context B-Task
integration I-Task
. O

In O
this O
paper O
, O
we O
propose O
an O
architecture O
to O
learn O
multilingual O
sentence B-Method
embeddings I-Method
for O
93 O
languages O
. O

paragraph O
: O
Distributed B-Method
Stochastic I-Method
Gradient I-Method
Descent I-Method

Our O
goal O
is O
to O
learn O
a O
target B-Method
representation I-Method
, O
and O
classifier B-Method
that O
can O
correctly O
classify O
target O
images O
into O
one O
of O
categories O
at O
test O
time O
, O
despite O
the O
lack O
of O
in O
domain O
annotations O
. O

section O
: O
Pose O
- O
driven O
Deep O
ReID B-Task
Model O

Experience B-Task
replay I-Task
may O
also O
help O
to O
prevent O
overfitting O
by O
allowing O
the O
agent O
to O
learn O
from O
data O
generated O
by O
previous O
versions O
of O
the O
policy O
. O

We O
can O
use O
this O
case O
to O
gain O
some O
intuition O
for O
how O
adversarial O
examples O
are O
generated O
in O
a O
simple O
setting O
. O

The O
object O

We O
propose O
a O
deep B-Method
convolutional I-Method
neural I-Method
network I-Method
architecture I-Method
codenamed I-Method
âInceptionâ I-Method
, O
which O
was O
responsible O
for O
setting O
the O
new O
state O
of O
the O
art O
for O
classification B-Task
and O
detection B-Task
in O
the O
ImageNet B-Task
Large I-Task
- I-Task
Scale I-Task
Visual I-Task
Recognition I-Task
Challenge I-Task
2014 I-Task
( O
ILSVRCâ14 B-Task
) O
. O

Thus O
, O
a O
large O
, O
regularized B-Method
LSTM I-Method
LM I-Method
, O
with O
projection B-Method
layers I-Method
and O
trained O
with O
an O
approximation O
to O
the O
true O
Softmax B-Method
with O
importance B-Method
sampling I-Method
performs O
much O
better O
than O
N B-Method
- I-Method
grams I-Method
. O

Our O
replications O
of O
these O
baseline O
networks O
are O
in O
Table O
[ O
reference O
] O

If O
the O
training O
set O
has O
examples O
, O
is O
bounded O
as O
follows O
( O
see O
surgery O
) O
. O

We O
also O
applied O
dropout B-Method
to O
the O
last O
convolutional B-Method
layer I-Method
( O
that O
is O
, O
before O
the O
matrix O
) O
. O

A O
major O
reason O
is O
that O
most O
similarity B-Metric
scores I-Metric
based O
on O
word B-Method
embeddings I-Method
are O
positive O
. O

The O
log O
probabilities O
from O
both O
models O
are O
added O
. O

, O
the O
14 O
located O
body O
joints O
are O
assigned O
to O
six O
rectangles O
indicating O
six O
parts O
. O

The O
cost O
of O
computing O
tensor O
( O
as O
shown O
in O
Figure O
[ O
reference O
] O
) O
is O
time O
. O

Our O
baseline O
is O
adapted O
from O
ResNet50 B-Method
with I-Method
dilated I-Method
network I-Method
, O
which O
yields O
MeanIoU B-Metric
34.28 I-Metric
and O
Pixel B-Metric
Acc I-Metric
. O

On O
BLEU B-Metric
score I-Metric
, O
the O
MoE B-Method
model O
significantly O
beats O
the O
multilingual O
GNMT B-Method
model O
on O
11 O
of O
the O
12 O
language O
pairs O
( O
by O
as O
much O
as O
5.84 O
points O
) O
, O
and O
even O
beats O
the O
monolingual O
GNMT B-Method
models O
on O
8 O
of O
12 O
language O
pairs O
. O

To O
test O
this O
hypothesis O
, O
we O
trained O
an O
ensemble B-Method
of I-Method
twelve I-Method
maxout I-Method
networks I-Method
on O
MNIST B-Material
. O

In O
early O
epochs O
, O
as O
we O
move O
from O
the O
output O
layer O
to O
the O
input O
layer O
, O
we O
observe O
on O
the O
model O
larger O
variance O
across O
all O
layers O
, O
suggesting O
that O
learning B-Task
is O
indeed O
occurring O
in O
all O
the O
layers O
thanks O
to O
intermediate O
supervision O
. O

Table O
[ O
reference O
] O
shows O
that O
both O
variants O
of O
our O
method O
outperform O
the O
other O
methods O
significantly O
. O

. O

[ O
10 O
] O
learn O
invariant O
features O
from O
video O
by O
enforcing O
a O
temporal O
slowness O
constraint O
on O
the O
feature B-Method
representation I-Method
learned O
by O
a O
linear B-Method
autoencoder I-Method
. O

( O
mp O
- O
6 O
) O
– O
( O
m O
- O
8 O
) O
; O
arrow O
, O
shorten O
¿ O
= O
18pt O
] O

we O
follow O
the O
naming O
convention O
used O
in O
Johnson O
el O
al O
. O

Given O
any O
view O
( O
window O
) O
in O
the O
image O
, O
we O
map O
this O
window O
to O
the O
feature O
maps O
( O
the O
way O
of O
mapping O
is O
in O
Appendix O
) O
, O
and O
then O
use O
SPP B-Method
to O
pool O
the O
features O
from O
this O
window O
( O
see O
Figure O
[ O
reference O
] O
) O
. O

Based O
on O
a O
given O
LR B-Task
image O
, O
how O
to O
achieve O
an O
approximated B-Task
HR I-Task
image I-Task
̂ I-Task
is O
a O
classic O
inverse B-Task
problem I-Task
, O
which O
requires O
priors O
based O
on O
the O
Bayesian B-Method
theory I-Method
. O

Due O
to O
the O
simple O
form O
of O
aggregate O
channel O
features O
and O
fast O
computation O
of O
feature B-Method
pyramid I-Method
, O
detection B-Task
is O
quite O
efficient O
. O

Improved O
adversarial B-Metric
loss I-Metric

All O
reports O
the O
evaluation O
over O
the O
whole O
testing O
set O
; O
Inter%20 O
reports O
the O
evaluation O
over O
the O
sub O
- O
set O
containing O
the O
images O
with O
top O
20 O
% O
interaction O
intensity O
; O
Inter%10 O
reports O
the O
evaluation O
over O
the O
sub O
- O
set O
containing O
the O
images O
with O
top O
10 O
% O
interaction O
intensity O
. O

Visualization B-Task
. O

• O

So O
as O
to O
obtain O
a O
more O
complete O
picture O
of O
the O
behavior O
of O
our O
multilingual O
sentence B-Task
representations I-Task
, O
we O
also O
evaluate O
them O
in O
cross B-Task
- I-Task
lingual I-Task
document I-Task
classification I-Task
( O
MLDoc B-Material
, O
Section O
[ O
reference O
] O
) O
, O
and O
bitext B-Task
mining I-Task
( O
BUCC B-Material
, O
Section O
[ O
reference O
] O
) O
. O

In O
essence O
, O
our O
formulation O
of O
MT O
- O
Tri B-Method
is O
closer O
to O
the O
original O
tri B-Method
- I-Method
training I-Method
formulation I-Method
( O
agreements O
on O
two O
provide O
pseudo O
- O
labels O
to O
the O
third O
) O
thereby O
incorporating O
more O
diversity O
. O

Although O
they O
have O
achieved O
high O
performance O
, O
they O
may O
either O
fail O
to O
fully O
make O
use O
of O
the O
syntactical O
information O
in O
sentences O
or O
be O
difficult O
to O
train O
due O
to O
the O
long O
propagation O
path O
. O

Again O
, O
let O
denote O
an O
odd O
number O
, O
or O
collection O
of O
odd O
numbers O
, O
e.g. O
, O
or O
. O

In O
this O
paper O
, O
we O
adopt O
a O
variant O
of O
channel O
features O
called O
aggregate B-Method
channel I-Method
features I-Method
, O
which O
are O
extracted O
directly O
as O
pixel O
values O
on O
subsampled O
channels O
. O

Moreover O
, O
they O
may O
need O
additional O
NLP B-Method
parsers I-Method
or O
external O
knowledge O
sources O
that O
may O
not O
be O
available O
for O
some O
languages O
. O

To O
verify O
the O
effectiveness O
of O
our O
model O
and O
proposed O
method O
, O
we O
conduct O
extensive O
experiments O
on O
popular O
face O
detection O
datasets O
, O
including O
WIDER B-Material
FACE I-Material
, O
FDDB B-Material
, O
Pascal B-Material
Faces I-Material
and O
AFW B-Material
. O

This O
measures O
the O
training B-Metric
set I-Metric
loss I-Metric
in O
a O
way O
that O
does O
not O
depend O
on O
the O
example B-Method
sampling I-Method
scheme I-Method
. O

While O
their O
goal O
is O
quite O
different O
( O
building O
generative B-Method
deep I-Method
networks I-Method
that O
can O
synthesize O
samples O
) O
, O
the O
way O
they O
measure O
and O
minimize O
the O
discrepancy O
between O
the O
distribution O
of O
the O
training O
data O
and O
the O
distribution O
of O
the O
synthesized O
data O
is O
very O
similar O
to O
the O
way O
our O
architecture O
measures O
and O
minimizes O
the O
discrepancy O
between O
feature O
distributions O
for O
the O
two O
domains O
. O

This O
work O
was O
in O
part O
supported O
by O
ARC O
Future O
Fellowship O
( O
FT120100969 O
) O
. O

An O
understanding B-Task
of I-Task
human I-Task
posture I-Task
and I-Task
limb I-Task
articulation I-Task
is O
important O
for O
high B-Task
level I-Task
computer I-Task
vision I-Task
tasks I-Task
such O
as O
human B-Task
action I-Task
or I-Task
activity I-Task
recognition I-Task
, O
sports B-Task
analysis I-Task
, O
augmented B-Task
and I-Task
virtual I-Task
reality I-Task
. O

Although O
( O
for O
example O
) O
LDCF8 B-Method
uses O
only O
of O
the O
number O
of O
filters O
per O
channel O
compared O
to O
Checkerboards4x4 O
, O
due O
to O
the O
step O
size O
increase O
, O
the O
obtained O
feature B-Metric
vector I-Metric
size I-Metric
is O
. O

The O
work O
of O
lei O
- O
EtAl:2014:P14 O
- O
1 O
suggests O
a O
low B-Method
- I-Method
rank I-Method
tensor I-Method
representation I-Method
to O
automatically O
find O
good O
feature O
combinations O
. O

We O
partition O
our O
meta O
- O
training O
set O
and O
meta O
- O
validation O
set O
to O
337 O
/ O
50 O
respectively O
. O

G O
f O
( O
· O
) O
. O

[ O
width O
= O
trim=0pt O
0 O

theorem O
: O

ANR B-Method
replaces O
the O
sparse B-Method
- I-Method
decomposition I-Method
optimization I-Method
( O
1 O
- O
norm O
) O
with O
a O
ridge B-Method
regression I-Method
( O
i.e. O
2 O
- O
norm O
) O
, O
where O
the O
coefficients O
can O
be O
computed O
offline O
and O
each O
coefficient O
can O
be O
stored O
as O
an O
atom O
( O
anchor O
) O
in O
the O
dictionary O
. O

Older O
semi B-Method
- I-Method
supervised I-Method
learning I-Method
algorithms I-Method
like O
self B-Method
- I-Method
training I-Method
do O
not O
suffer O
from O
this O
problem O
because O
they O
continually O
learn O
about O
a O
task O
on O
a O
mix O
of O
labeled O
and O
unlabeled O
data O
. O

−1 O
otherwise O
, O
i.e. O
= O
( O
ℛ O
) O
∈ O
{ O
−1 O
, O
1 O
} O
. O

← O
σ O

Our O
approach O
is O
simple O
yet O
surprisingly O
powerful O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
visual B-Task
adaptation I-Task
results O
on O
the O
MNIST B-Material
, O
USPS B-Material
, O
and O
SVHN B-Material
digits O
datasets O
. O

All O
parameter O
details O
are O
described O
in O
section O
[ O
reference O
] O
, O
and O
kept O
identical O
across O
experiments O
unless O
explicitly O
stated O
. O

As O
an O
RNN B-Method
- O
based O
model O
, O
QRN B-Method
is O
a O
single O
recurrent B-Method
unit I-Method
that O
updates O
its O
hidden O
state O
( O
reduced O
query O
) O
through O
time O
and O
layers O
. O

A O
few O
representative O
images O
are O
shown O
in O
Fig O
. O

We O
randomly O
sample O
different O
percentages O
of O
the O
labeled O
documents O
for O
training O
and O
use O
the O
rest O
for O
evaluation B-Task
. O

Scoring B-Task
is O
performed O
using O
an O
MLP B-Method
with O
one O
hidden B-Method
layer I-Method
, O
resulting O
in O
matrix B-Method
- I-Method
vector I-Method
multiplications I-Method
from O
the O
input O
to O
the O
hidden O
layer O
, O
and O
multiplications O
from O
the O
hidden O
to O
the O
output O
layer O
. O

In O
order O
to O
incorporate O
the O
future O
dependency O
, O
we O
obtain O
− O
→ O
h O
t O
and O
← O

theorem O
: O

The O
dimensionality O
of O
the O
embedding B-Method
layer O
, O
and O
the O
input O
and O
output O
dimensionality O
of O
the O
MoE B-Method
layer O
are O
set O
to O
1024 O
instead O
of O
512 O
. O

The O
first O
model O
is O
a O
random B-Method
token I-Method
generation I-Method
. O

which O
is O
equivalent O
to O
independently O
solving O
the O
following O
sub B-Task
- I-Task
optimization I-Task
problem I-Task
for O
each O
= O
1 O
, O
⋯ O
, O
and O
= O
1 O
, O
⋯ O
, O
via O
: O

The O
novel O
properties O
of O
the O
proposed O
model O
and O
our O
contributions O
can O
be O
summarized O
as O
follows O
: O

and O
then O
up O
- O
sample O
it O
to O
its O
original O
size O
, O
obtaining O
a O
low O
- O
resolution O
version O
as O
the O
input O
of O
the O
network O
. O

This O
flexibility O
allows O
QR B-Method
- I-Method
DQN I-Method
to O
significantly O
improve O
on O
C51 O
’s O
Atari B-Task
- I-Task
57 I-Task
performance O
. O

We O
first O
formally O
define O
the O
base B-Method
model I-Method
of O
a O
QRN B-Method
unit O
, O
and O
then O
we O
explain O
how O
we O
connect O
the O
input O
and O
output O
modules O
to O
it O
( O
Section O
2.1 O
) O
. O

Actor O
{ O
algorithmic} O
[ O
1 O
] O
T O
agent O
in O
environment O
instance O
, O
storing O
experiences O
. O

As O
will O
be O
shown O
in O
section O
[ O
reference O
] O
, O
our O
implementation O
( O
LDCF B-Method
- I-Method
Ours I-Method
) O
clearly O
improves O
over O
the O
previously O
published O
numbers O
, O
showing O
the O
potential O
of O
the O
method O
. O

The O
application O
of O
channel B-Method
has O
a O
long O
history O
since O
digital O
images O
were O
invented O
. O

Compared O
to O
our O
initial O
tests O
using O
a O
sliding B-Method
window I-Method
, O
this O
ap O
- O
proach O
improved O
our O
2D B-Task
detection I-Task
results O
from O
about O
75 O
% O
to O
98.8 O
% O
correct B-Metric
detection I-Metric
rate I-Metric
based O
on O
a O
IoU B-Metric
of I-Metric
0.5 I-Metric
. O

The O
general O
approximated B-Method
nearest I-Method
neighbor I-Method
( O
ANN B-Method
) O
search O
framework O
[ O
reference O
][ O
reference O
] O
is O
an O
efficient O
strategy O
for O
large B-Task
- I-Task
scale I-Task
image I-Task
retrieval I-Task
, O
which O
mainly O
consists O
of O
4 O
parts O
: O
( O
1 O
) O
extracting O
compact O
features O
( O
e.g. O
, O
locality B-Method
- I-Method
sensitive I-Method
Hashing I-Method
( O
LSH B-Method
) O
[ O
reference O
] O
feature O
) O
for O
a O
query O
image O
; O
( O
2 O
) O
coarse B-Method
- I-Method
level I-Method
search I-Method
using O
Hamming O
distance O
to O
measure O
the O
similarity B-Metric
between O
binary O
compact O
Hash O
features O
, O
then O
narrow O
the O
search O
scope O
into O
a O
smaller O
candidate O
group O
; O
( O
3 O
) O
fine B-Method
- I-Method
level I-Method
search I-Method
by O
using O
Euclidean B-Method
distance I-Method
to O
measure O
the O
similarity O
between O
their O
corresponding O
feature O
vectors O
; O
and O
( O
4 O
) O
finding O
the O
object O
in O
the O
smaller O
candidate O
group O
that O
is O
the O
nearest O
one O
to O
the O
query O
image O
. O

( O
mi O
- O
15 O
) O

Table O
[ O
reference O
] O
and O
Figure O
[ O
reference O
] O
show O
results O
and O
localization B-Task
examples O
on O
COCO B-Material
dataset I-Material
. O

For O
MoE B-Method
- O
34 O
M O
, O
the O
LSTM B-Method
layers O
have O
1024 O
units O
. O

Three O
types O
of O
prediction B-Metric
values I-Metric
are O
evaluated O
: O
pitch O
, O
yaw O
, O
and O
pitch O
+ O
yaw O
, O
where O
pitch O
+ O
yaw O
jointly O
estimates O
the O
pitch O
and O
yaw O
angles O
. O

section O
: O
Algorithm O
Workflow O

Given O
the O
general O
architecture O
and O
the O
baselines O
described O
in O
section O
[ O
reference O
] O
, O
we O
now O
proceed O
to O
explore O
different O
types O
of O
filter B-Method
banks I-Method
. O

subsection O
: O
Evaluation O

He O
was O
an O
Assistant O
Professor O
with O
the O
Nanyang O
Technological O
University O
, O
Singapore O
. O

( O
b O
) O
. O

Both O
results O
outperform O
previous O
works O
substantially O
in O
terms O
of O
image B-Metric
quality I-Metric
. O

We O
evaluate O
PSPNet B-Method
with O
several O
- O
scale O
input O
and O
use O
the O
average O
results O
following O
. O

¡ O
= O
- O
2pt O
] O

Experiments O
show O
that O
this O
is O
one O
reason O
for O
the O
gain O
of O
accuracy B-Metric
. O

: O
Detailed O
results O
of O
the O
proposed O
method O
, O
compared O
with O
state O
- O
of O
- O
the O
- O
art O
methods O
on O
the O
dataset O
Set5 B-Material
, O
in O
terms O
of O
PSNR B-Metric
( O
dB O
) O
using O
three O
different O
magnification O
factors O
( O
×2 O
, O
×3 O
, O
×4 O
) O
. O

On O
the O
contrary O
, O
the O
MAE B-Metric
of O
DLDL B-Method
reduces O
quickly O
. O

Removal O
of O
sentences O
that O
appear O
multiple O
times O
, O
either O
in O
the O
source O
or O
the O
target O
. O

( O
x O
) O
component O
- O
wise O
with O
a O
sparse B-Method
mask I-Method
M I-Method
( O
G O
σ O
( O
x O
) O
) O
and O
normalize O
the O
output O
. O

When O
no O
proposal B-Method
is O
selected O
, O
we O
preserve O
scores O
from O
the O
proposal B-Method
network I-Method
without O
calibration B-Method
as O
they O
are O
typically O
low O
. O

2 O
) O

document O
: O
CoupleNet B-Method
: O
Coupling O
Global O
Structure O
with O
Local O
Parts O
for O
Object B-Task
Detection I-Task

Note O
that O
our O
cascade B-Method
architecture I-Method
offers O
a O
natural O
way O
to O
select O
the O
challenging O
instances O
from O
such O
incoming O
images O
. O

We O
found O
applying O
dropout B-Method
to O
the O
ELMo B-Method
embeddings O
was O
crucial O
for O
achieving O
good O
performance O
. O

section O
: O
Datasets O
and O
Quantitative B-Task
Analysis I-Task

The O
DeepFace B-Method
approach I-Method
uses O
a O
carefully O
crafted O
3D B-Method
alignment I-Method
procedure I-Method
to O
preprocess O
face O
images O
and O
feeds O
them O
to O
a O
deep B-Method
network I-Method
that O
is O
trained O
using O
a O
large O
training O
set O
. O

Noise B-Method
Contrastive I-Method
Estimation I-Method
( O
NCE B-Method
) O
proposes O
to O
consider O
a O
surrogate B-Task
binary I-Task
classification I-Task
task I-Task
in O
which O
a O
classifier B-Method
is O
trained O
to O
discriminate O
between O
true O
data O
, O
or O
samples O
coming O
from O
some O
arbitrary O
distribution O
. O

A O
common O
practice O
in O
deep B-Task
learning I-Task
literature I-Task
is O
to O
employ O
non B-Method
- I-Method
linear I-Method
activation I-Method
functions I-Method
on O
hidden O
neurons O
. O

The O
LINE B-Method
( O
2nd O
) O
performs O
quite O
well O
and O
generates O
meaningful O
layout O
of O
the O
network O
( O
nodes O
with O
same O
colors O
are O
distributed O
closer O
) O
. O

Since O
the O
release O
of O
the O
IJB B-Material
- I-Material
A I-Material
dataset I-Material
, O
there O
have O
been O
several O
works O
that O
have O
published O
verification B-Task
results O
for O
this O
dataset O
. O

We O
first O
discuss O
the O
architecture O
of O
our O
detector O
in O
Section O
[ O
reference O
] O
, O
then O
we O
elaborate O
our O
hard B-Method
image I-Method
mining I-Method
strategy I-Method
in O
Section O
[ O
reference O
] O
, O
as O
well O
as O
some O
other O
useful O
training B-Method
techniques I-Method
in O
Section O
[ O
reference O
] O
. O

Each O
sentence O
was O
padded O
to O
form O
a O
full O
binary O
tree B-Method
. O

However O
, O
in O
experiment O
6 O
, O
when O
we O
only O
remove O
fuse O
gate O
, O
to O
our O
surprise O
, O
the O
performance O
degrade O
to O
73.5 O
for O
matched B-Metric
score I-Metric
and O
73.8 O
for O
mismatched O
. O

However O
, O
it O
is O
important O
to O
note O
that O
in O
this O
dataset O
premise O
- O
hypotheses O
pairs O
were O
not O
generated O
directly O
with O
reference O
to O
the O
images O
themselves O
. O

However O
, O
while O
the O
parser B-Method
of O
Vinyals O
et O
al O
. O
relies O
on O
a O
trainable B-Method
attention I-Method
mechanism I-Method
for O
focusing O
on O
specific O
BiLSTM O
vectors O
, O
parsers B-Method
in O
the O
transition B-Method
- I-Method
based I-Method
family I-Method
we O
use O
in O
Section O
[ O
reference O
] O
use O
a O
human B-Method
designed I-Method
stack I-Method
and I-Method
buffer I-Method
mechanism I-Method
to O
manually O
direct O
the O
parser O
’s O
attention O
. O

Moreover O
, O
a O
softmax B-Method
output I-Method
layer I-Method
maps O
the O
hidden O
states O
into O
the O
output O
token O
distribution O
where O
the O
parameters O
are O
a O
bias O
vector O
and O
a O
weight O
matrix O
. O

The O
pyramid O
is O
{ O
6 O
6 O
, O
3 O
3 O
, O
2 O
2 O
, O
1 O
1 O
} O
( O
totally O
50 O
bins O
) O
. O

a O
uniform O
square O
, O
all O
horizontal O
and O
vertical O
gradient O
detectors O
( O
values O
) O
, O
and O
all O
possible O
checkerboard O
patterns O
. O

In O
general O
, O
we O
note O
that O
tri B-Method
- I-Method
training I-Method
works O
best O
on O
OOVs B-Material
and O
on O
low O
- O
frequency O
tokens O
, O
which O
is O
also O
shown O
in O
Figure O
3 O
( O
leftmost O
bins O
) O
. O

( O
ai O
- O
17 O
) O
– O
( O
ai O
- O
18 O
) O
; O
[ O
green O
hbox O
, O
right O
of O
= O

by O
approximately O
( O
by O
mm O
) O
. O

We O
assume O
is O
complete O
, O
i.e. O
, O
any O
possible O
value O
has O
a O
corresponding O
member O
in O
. O

To O
achieve O
this O
goal O
, O
we O
take O
model O
snapshots O
from O
each O
method O
every O
20k O
steps O
of O
optimization B-Task
and O
run O
them O
over O
the O
entire O
VOC07 B-Material
trainval O
set O
to O
compute O
the O
average B-Metric
loss I-Metric
over O
all O
RoIs O
. O

PCA B-Method
or O
the O
LSH B-Method
compressed O
features O
, O
to O
perform O
ridge B-Method
regression I-Method
in O
the O
leaf O
nodes O
. O

( O
First O
- O
order O
Proximity O
) O

, O
sit;θ O
) O
Rm O
be O
the O
prediction B-Method
function I-Method
as O
described O
above O
. O

Since O
the O
tasks O
involve O
classifying O
images O
that O
have O
two O
attributes O
, O
this O
task O
is O
ambiguous O
, O
and O
there O
are O
three O
possible O
combinations O
of O
two O
attributes O
that O
explain O
the O
training O
set O
. O

For O
example O
, O
to O
mitigate O
the O
information B-Task
flow I-Task
bottleneck I-Task
, O
bahdanau:15 O
extended O
RNNs B-Method
with O
a O
soft B-Method
attention I-Method
mechanism I-Method
in O
the O
context O
of O
neural B-Task
machine I-Task
translation I-Task
, O
leading O
to O
improved O
the O
results O
in O
translating O
longer O
sentences O
. O

YOLO B-Method
has O
good O
performance O
on O
VOC B-Material
2007 I-Material
and O
its O
AP B-Metric
degrades O
less O
than O
other O
methods O
when O
applied O
to O
artwork O
. O

In O
this O
sub O
- O
section O
, O
we O
further O
analyze O
the O
ridge B-Method
regression I-Method
employed O
in O
the O
RF B-Method
leaf O
nodes O
. O

Models O
which O
can O
accurately O
place O
distributions O
over O
sentences O
not O
only O
encode O
complexities O
of O
language O
such O
as O
grammatical O
structure O
, O
but O
also O
distill O
a O
fair O
amount O
of O
information O
about O
the O
knowledge O
that O
a O
corpora O
may O
contain O
. O

Quantifier O
, O
Pronoun O
, O
Diff O
Tense O
, O
Superlative O
and O
Bare O
NP O
were O
identified O
using O
Penn O
treebank O
labels O
, O
while O
labels O
such O
as O
Negation O
were O
found O
with O
a O
straightforward O
keyword B-Method
search I-Method
. O

[ O
black O
, O
midway O
, O
xshift= O
- O
1 O
cm O
, O
rotate=90 O
, O
anchor O
= O
north O
] O
Matching O
layer O
; O
[ O
decorate O
, O
thick O
, O
decoration O
= O
brace O
, O
amplitude=6pt O
, O
yshift=0pt O
] O

subsection O
: O
Decoding B-Task

We O
apply O
the O
standard O
hard B-Method
negative I-Method
mining I-Method
to O
train O
the O
SVM B-Method
. O

the O
VGG B-Method
- I-Method
Nets I-Method
with O
square B-Method
loss I-Method
and O
multi B-Method
- I-Method
label I-Method
cross I-Method
- I-Method
entropy I-Method
loss I-Method
and O
use O
them O
as O
our O
IF B-Method
- I-Method
DLDL I-Method
’s I-Method
baselines I-Method
. O

, O
2http: O
// O
host.robots.ox.ac.uk:8080 O
/ O
anonymous O
/ O
H49PTT.html O
, O
3http: O
// O
host.robots.ox.ac.uk:8080 O
/ O
anonymous O

Let O
denote O
the O
annotation O
of O
the O
- O
th O
pixel O
, O
where O
( O
assuming O
there O
are O
categories O
and O
0 O
for O
background O
) O
. O

The O
assumption O
behind O
the O
convolutional B-Method
architecture I-Method
is O
that O
memorising O
erroneous O
token O
sequences O
from O
the O
training O
data O
is O
sufficient O
for O
performing O
error B-Task
detection I-Task
. O

|. O

Given O
an O
input O
image O
, O
we O
extract O
candidate O
proposals O
by O
using O
the O
Region B-Method
Proposal I-Method
Network I-Method
( O
RPN B-Method
) I-Method
, O
which O
also O
shares O
convolution O
features O
with O
CoupleNet B-Method
following O
. O

We O
optionally O
employ O
a O
fully B-Method
connected I-Method
CRF I-Method
to O
refine O
the O
predicted O
category O
score O
maps O
using O
the O
default O
parameters O
of O
. O

Our O
method O
generalizes O
well O
across O
a O
variety O
of O
tasks O
, O
achieving O
strong O
results O
on O
benchmark O
adaptation O
datasets O
as O
well O
as O
a O
challenging O
cross B-Task
- I-Task
modality I-Task
adaptation I-Task
task I-Task
. O

In O
addition O
, O
the O
CoNLL B-Method
systems I-Method
are O
mostly O
combinations O
of O
many O
alternative O
models O
: O
the O
CAMB B-Method
system I-Method
is O
a O
hybrid O
of O
machine B-Method
translation I-Method
, O
a O
rule B-Method
- I-Method
based I-Method
system I-Method
, O
and O
a O
language B-Method
model I-Method
re I-Method
- I-Method
ranker I-Method
; O
CUUI B-Method
consists O
of O
different O
classifiers B-Method
for O
each O
individual O
error O
type O
; O
and O
P1 B-Method
+ I-Method
P2 I-Method
+ I-Method
S1 I-Method
+ I-Method
S2 I-Method
is O
a O
combination O
of O
four O
different O
error B-Method
correction I-Method
systems I-Method
. O

The O
Rightℓ O
transition O
removes O
from O
the O
stack O
and O
attaches O
it O
as O
a O
modifier O
to O
the O
next O
item O
on O
the O
stack O
( O
) O
, O
adding O
the O
arc O
. O

In O
distributional O
RL B-Method
, O
the O
distribution O
over O
returns O
( O
the O
law O
of O
) O
is O
considered O
instead O
of O
the O
scalar O
value O
function O
that O
is O
its O
expectation O
. O

[ O
reference O
] O
) O
. O

To O
address O
this O
issue O
, O
knowledge B-Method
transfer I-Method
or O
domain B-Method
adaptation I-Method
techniques O
have O
been O
proposed O
to O
close O
the O
gap O
between O
source O
and O
target O
domains O
, O
where O
annotations O
are O
not O
available O
in O
the O
target O
domain O
. O

In O
our O
work O
, O
we O
train O
models O
on O
the O
popular O
One B-Material
Billion I-Material
Word I-Material
Benchmark I-Material
, O
which O
can O
be O
considered O
to O
be O
a O
medium O
- O
sized O
data O
set O
for O
count O
- O
based O
LMs B-Task
but O
a O
very O
large O
data O
set O
for O
NN O
- O
based O
LMs B-Task
. O

[ O
blue O
(- O
3 O
, O
- O
12 O
) O
] O

The O
transition B-Method
system I-Method
has O
a O
set O
of O
configurations O
and O
a O
set O
of O
transitions O
which O
are O
applied O
to O
configurations O
. O

Contextual O
information O
from O
global O
and O
local O
regions O
compensates O
each O
other O
and O
naturally O
benefits O
human B-Task
parsing I-Task
. O

We O
observe O
that O
the O
4 B-Method
layer I-Method
LSTM I-Method
is O
significantly O
better O
than O
the O
DNN B-Method
and O
the O
two O
single B-Method
layer I-Method
LSTMs I-Method
trained O
on O
bottleneck O
features O
. O

Yet O
there O
is O
still O
much O
room O
to O
exploit O
necessary O
information O
in O
complex O
scenes O
. O

One O
instance O
of O
such O
architecture O
, O
Densely B-Method
Interactive I-Method
Inference I-Method
Network I-Method
( O
DIIN B-Method
) O
, O
demonstrates O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
large O
scale O
NLI B-Task
copora O
and O
large O
- O
scale O
NLI B-Task
alike O
corpus O
. O

We O
treat O
each O
body O
joint O
of O
each O
person O
as O
a O
tracking O
target O
and O
measure O
tracking B-Task
performance O
using O
a O
standard O
multiple B-Metric
object I-Metric
tracking I-Metric
accuracy I-Metric
( O
MOTA B-Metric
) I-Metric
metric I-Metric
that O
incorporates O
identity O
switches O
, O
false B-Metric
positives I-Metric
and O
false B-Metric
negatives I-Metric
. O

We O
perform O
parameter B-Task
sensitivity I-Task
analysis I-Task
of O
our O
proposed O
model O
aNMM B-Method
. O

These O
methods O
have O
shown O
that O
CNN B-Method
descriptors O
can O
boost O
performance O
against O
traditional O
hand O
- O
designed O
features O
. O

De B-Method
- I-Method
convolutional I-Method
layers I-Method
are O
then O
used O
to O
recover O
the O
image O
details O
. O

However O
, O
subsequent O
work O
has O
not O
operated O
at O
a O
part O
level O
. O

CUHK03 O
only O
contains O
two O
camera O
views O
. O

The O
column O
" O
Domain B-Task
Classification I-Task
" O
shows O
the O
decision O
boundary O
on O
the O
domain B-Task
classification I-Task
problem I-Task
, O
which O
is O
given O
by O
the O
domain B-Method
regressor I-Method
G I-Method
d I-Method
of O
Equation O
( O
7 O
) O
. O

We O
see O
that O
the O
method O
is O
able O
to O
handle O
non O
- O
standard O
poses O
and O
resolve O
ambiguities O
between O
symmetric O
parts O
for O
a O
variety O
of O
different O
relative O
camera O
views O
. O

In O
a O
supervised B-Task
learning I-Task
task I-Task
, O
the O
weighting O
of O
dimensions O
can O
be O
automatically O
found O
based O
on O
the O
training O
data O
. O

In O
this O
setting O
it O
is O
important O
to O
compare O
against O
the O
re B-Method
- I-Method
trained I-Method
MLP I-Method
rather O
than O
the O
jointly B-Method
trained I-Method
reference I-Method
model I-Method
, O
as O
training O
on O
features O
extracted O
from O
fixed O
convolutional O
layers O
typically O
leads O
to O
lower O
performance O
than O
joint B-Method
training I-Method
. O

In O
structured B-Task
variational I-Task
inference I-Task
, O
we O
approximate O
the O
distribution O
over O
the O
hidden O
variables O
θ O
and O
φ O
i O
for O
each O
task O
with O
some O
approximate O
distribution O
q O
i O
( O
θ O
, O
φ O
i O
) O
. O

Whenever O
an O
output O
site O
is O
determined O
to O
be O
active O
, O
its O
output O
feature O
vector O
is O
calculated O
by O
the O
SC B-Method
operation O
. O

We O
use O
the O
distance O
for O
the O
NN B-Task
retrieval I-Task
. O

However O
, O
when O
used O
for O
multi B-Task
- I-Task
hop I-Task
reasoning I-Task
in O
question B-Task
answering I-Task
, O
purely O
RNN B-Method
- O
based O
models O
have O
shown O
to O
perform O
poorly O
. O

[ O
reference O
] O
. O

Then O
, O
the O
output O
of O
the O
cross B-Method
network I-Method
is O
a O
scalar O
multiple O
of O
. O

These O
two O
types O
of O
features O
are O
then O
fused O
under O
a O
unified B-Method
framework I-Method
for O
multi B-Task
- I-Task
class I-Task
person I-Task
identification I-Task
. O

subsubsection O
: O
Early B-Method
fusion I-Method

The O
HyperFace B-Method
detector I-Method
automatically O
extracts O
many O
faces O
from O
a O
given O
image O
. O

Since O
SGDand B-Method
its O
many O
variants O
, O
such O
as O
ADAGRAD B-Method
[ O
reference O
] O
or O
ADADELTA B-Method
[ O
reference O
]- O
is O
the O
main O
learning B-Method
algorithm I-Method
implemented O
in O
most O
libraries O
for O
deep B-Method
learning I-Method
, O
it O
would O
be O
convenient O
to O
frame O
an O
implementation O
of O
our O
stochastic B-Method
saddle I-Method
point I-Method
procedure I-Method
as O
SGD B-Method
. O

Dr. O
Qi O
Tian O
is O
supported O
by O
ARO O
grant O
W911NF O
- O
15 O
- O
1 O
- O
0290 O
and O
Faculty O
Research O
Gift O
Awards O
by O
NEC O
Laboratories O
of O
America O
and O
Blippar O
. O

– O

In O
Feature B-Method
Embedding I-Method
sub I-Method
- I-Method
Net I-Method
, O
each O
body O
part O
region O
is O
first O
automatically O
cropped O
. O

. O

Next O
we O
present O
more O
results O
on O
the O
Microsoft O
COCO O
object B-Task
detection I-Task
dataset O
. O

If O
handled O
improperly O
, O
the O
performance O
degrades O
a O
lot O
. O

In O
contrast O
, O
in O
our O
proposed O
Adaptive B-Method
Fastfood I-Method
transform I-Method
, O
the O
diagonal O
matrices O
are O
learned O
by O
backpropagation B-Method
. O

( O
3 O
) O
Citation O
Networks O
. O